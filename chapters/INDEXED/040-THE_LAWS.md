<div style='page-break-after: always; break-after: always;'></div>

# 4: THE LAWS

###### Energy creates <u>movement</u>, <u>movement</u> follows <u>laws</u>, <u>laws</u> create <u>order</u>, and <u>order</u> directs <u>energy</u>.  Each level of <u>existence</u> expresses the Universal <u>laws</u> according to the context and local <u>laws</u> it exists within.

##### **Synopsis:** If <u>awareness</u> is an expression of <u>energy</u>, then we should expect to see these <u>laws</u> in action, but in a different context. Entropy is as much a philosophical <u>concept</u> as it is a physics <u>concept</u>, as it is the <u>law</u> of <u>balance</u>.  We look at entropy from various perspectives and in relation to <u>chaos</u>, the 2<sup>nd</sup> Law of Thermodynamics, inertia, <u>pattern</u>, and <u>oscillation</u>. These are tied together in Newton’s 2<sup>nd</sup> Law of Motion which is deconstructed to a simple <u>pattern</u>, expanded upon, and re-applied to various contexts such as <u>alchemy</u>, <u>electricity</u>, and relativity.  

##### **Keywords:** <u>energy</u>, entropy, <u>microstates</u>, <u>patterns</u>, systems, Newton, balance

<center><img src='../Images/psolids-3.png' style='width:100%'/></center>

## Entropy

There are many <u>laws</u> that describe how <u>energy</u> works.  From the 3<sup>rd</sup> century B.C. starting with the *<u>Archimedes</u> Principle* to the present <u>laws</u> of <u>quantum mechanics</u>, researchers have been compiling and updating a long list of <u>laws</u>.

One of these universal <u>laws</u> is *<u>energy</u> will always follow the path of least <u>resistance</u>.* This is called *entropy* and explains why water runs downhill to form rivers, why <u>electricity</u> works, why high pressure seeks low pressure, and why stuff breaks.   

In short, entropy measures the amount of <u>energy</u> *not* available for <u>work</u>.  Yes, it sounds (and is) confusing.  It’s like a cashier saying “Here’s your not 70&cent; change” as she hands you 30&cent; from the \$1 you gave her for a 70&cent; chicklet.  There is no such *thing* as entropy, as it measures the lack of something.  The entropy of something is like using someone’s age as a measure of how much <u>life</u> they have left; more equals less.  Why can’t we just measure how much <u>energy</u> something has rather than what it doesn’t have?  For the same <u>reason</u> we can’t start counting down from 82 years to 0 as a measure of how much <u>life</u> someone has left.  We don’t know what the final age of anyone will be, nor do we know what the final <u>energy</u> state of something will be.  So, instead of handing the cashier $1, you just say “charge it”.  She has no idea what you credit <u>balance</u> is, so instead of giving you change, she just adds the 70&cent; to your growing credit card debt. 

**A quick review of entropy&hellip;**

Classical entropy is a <u>thermodynamic</u> <u>concept</u> based on the observations of how heat, or <u>energy</u>, moves.  At its core, it is a measure of how many different ways something can occur, so there is no such *thing* as entropy.  It is a statistical <u>concept</u> only, but it is still real.  You don’t have -\$10, but you do have a debt of \$10.  Entropy is to <u>energy</u> what debt to value.  Using the credit card metaphor, imagine how many ways you can spend <u>money</u> in the Dubai Mall, the largest mall in the world (1,124,000 m<sup>2</sup>, or 157 football fields!).  Now imagine how many ways you can spend <u>money</u> in Mexican Hat, Utah.

<center><img src="../Images/mall.png" style="width:80%"/></center>

Your credit card has <u>extremely low entropy in Mexican Hat</u>, and extremely high entropy in the Dubai mall.  In this metaphor, entropy’s “how many ways something can occur” equates to how many ways you can increase your credit card debt until you are flat broke (high debt, high entropy).  Taking the metaphor further, how much “<u>work</u>” can your &dollar;300 limit credit card do in Dubai?  Not much, as you can maybe get a decent sandwich or a couple pairs of socks.  How much “<u>work</u>” can this same card do at the “last chance” gas station/convenience store for miles and miles in any direction? Quite a bit more, if we define “<u>work</u>” as “obtaining the resources needed to get home”.  On the other hand, let’s say your card had a \&dollar;1,000,000 limit and “<u>work</u>” is defined as “obtaining the resources to live luxuriously”.  Your card can do a lot of “<u>work</u>” in Dubai, but would be useless in Mexican Hat.  OK, we have taken some liberties in this metaphor, but the point is clear.  The ability to do “<u>work</u>” depends on the <u>scope</u> of the “<u>work</u>” and the context of where it is applied. The local entropy of your card is high in Dubai and low in Mexican Hat when “<u>work</u>” means one thing, and opposite when “<u>work</u>” means something else.  However, the *universal* entropy of your card remains the same, very high, as &dollar;300 of <u>potential</u> debt doesn’t do much “<u>work</u>” relative to the 226 trillion in debt floating around in the universe of <u>money</u>.  As <u>black holes</u> account for most of the total entropy in the universe, you can think of the banks and governments that control and own most of this debt as financial <u>black holes</u>.

In a very non-metaphorical example, imagine two simple chemical elements named A and B, each made of 4 particles with 6 bonds that hold them together.  In chemistry, <u>energy</u> is stored in the bonds.  This <u>energy</u> is measured in units called *<u>quanta</u>*.  In A there are 6 <u>quanta</u>, and in B there are 2 <u>quanta</u>, so A is a little hotter than B.  These <u>quanta</u> can move around and be in any arrangement in each object but the <u>energy</u> will stay the same because there will always be 6 <u>quanta</u> in A and 2 <u>quanta</u> in B.  The various configurations that these <u>quanta</u> can have are called *<u>microstates</u>*.

A microstate represents one possible arrangement.  For example, how many <u>microstates</u> exist when tossing 2 pennies?  There are 4:  HH, TH, HT, TT, which means each microstate has a 25% chance.  If the experiment required 10 tosses, then the experiment has 10 *macrostates*, with each macrostate having 4 <u>microstates</u>.  Another example of <u>microstates</u> is a poker hand.   In a deck of 52 cards there are 8.06<sup>67</sup> possible <u>microstates</u> for the deck and  2.6 million possible hands of 5 cards, which is 2.6 million <u>microstates</u>. Of those, only 40 of these <u>microstates</u> are part of the *macrostate* of *straight*-flush while over 1 million are part of the macrostate called *junk* (official <u>poker term for polite</u> <u>society</u>).  Any of the 2.6 million possible <u>microstates</u> is equally likely, but one macrostate has 40 <u>microstates</u> and one has 1,000,000 <u>microstates</u>, so there’s a 25000:1 chance you’ll get dealt a junk hand, thanks to entropy. 

Returning to our example, a new object C is created by putting A and B together.  Now, C has 8 <u>quanta</u> and 12 bonds, so there are many more <u>potential</u> <u>microstates</u>.  But which <u>microstates</u> are the most likely to occur?  The <u>microstates</u> that create the most <u>balanced</u> are the most likely to occur, just like when you untie a balloon, the high-pressure air inside the balloon is more likely to exit the balloon then outside air entering the balloon. 

<center><img src="../Images/microstate_objects.png" style="width:80%"/></center>

Energy will naturally <u>balance</u> itself out simply because there are more <u>microstates</u> available when the <u>quanta</u> is evenly distributed.  This is like saying a head/tail pair is more likely to occur because there are twice as many head/tail <u>microstates</u> because both coins have a head and a tail, or, if you have 10 red buckets and 1 green bucket outside, when it rains, there will be more water in the red buckets than the green.

This example uses the <u>energy</u> in chemical bonds, but <u>energy</u> can be distributed in other ways, such as the <u>radiation</u> emitted by the universe and which we can see in the *cosmic background <u>radiation</u>*.  Where does this <u>energy</u> go?  Nowhere. It just gets recycled because there is nothing to share the <u>energy</u> with outside of this universe.

The formula for entropy looks astonishingly simple, but like that other simple looking formula, *E=mc<sup>2</sup>*, it can get insanely complex.  The change in entropy (*&Delta;S*) is the change in heat (*Q*) divided by temperature (*T*), so *&Delta;S=&Delta;Q/T*.  The *state* of entropy (*S*) is simply the natural log (*ln*) of the <u>number</u> of <u>microstates</u> (*&Omega;*), so *S=ln(&Omega;)*.  In the <u>coin toss</u> example, there are 4 <u>microstates</u>, so *ln(4)=1.386*, or in the case of our elements, we’d use the maximum <u>number</u> of possible <u>microstates</u> per configuration, so *ln(15876)=9.672*.  However, these <u>numbers</u> are useless until we relate them to something real.  In <u>thermodynamics</u>, this <u>number</u> would be multiplied by the *Boltzmann Constant*, *k*, or *k<sub>B</sub>*, which is the ratio of <u>kinetic</u> <u>energy</u> to temperature as measured in <u>joules</u>.  In other areas, like economics, or <u>information</u> theory, that value would be different.  In the <u>conceptual</u>, philosophical, metaphysical, or thought experiment realms, there is no “real” value to apply, but this has more to do with the way we understand what is “real”, because the <u>conceptual</u>, philosophical, metaphysical, and thought experiment realms all use <u>energy</u> in various forms, but not all of those forms are known to us, and measuring them can be difficult, impossible, or meaningless.  

So, what system has the least <u>microstates</u>? A system with the least <u>energy</u>, like something at absolute 0 temperature, which is impossible.  The best we can do is get really, really close (the record so far is 0.0001 Kelvin).  On the other end, what system has the most <u>microstates</u>? The only system that can have all possible <u>microstates</u> in it&hellip; the Universe, which appears to have a finite amount of <u>energy</u> in it. 

To sum up, entropy measures the degree to which the <u>energy</u> of a system is <u>balanced</u>. More entropy means more dispersion of <u>energy</u>, more <u>balancing</u> of differences, low pressure.  Less entropy means less dispersion, more imbalance, more difference, high pressure.

***Continuing&hellip;***

<img src="../Images/grandcanyon.png" style="float:right;width:30%"/>OK, that was the thermodynamic explanation of entropy, and the one most people learn about, but here we will use the word more broadly to reflect a more general concept, which is the ratio *distribution of resources* over *availability of resources*, or $\frac{resource-distribution}{resources-available}$.  So, an abundance of resources with no where to go would be a very **low entropy** of $\frac{1}{1000000}$  vs. few resources that are distributed via many channels, which has a very **high entropy** of $\frac{1000000}{1}$.  In our broader context of the concept of entropy, this is similar to saying $\frac{kinetic-energy}{potential-energy}$ which would be saying that a large river with only 1 possible path, such as the *Grand Canyon*, would be a **low entropy** system, while a trickling stream that empties into a swamp is a **high-entropy** system.

We all know what “<u>work</u>” means when we say “That was a lot of <u>work</u>”, but if someone asked us “How many <u>joules</u> did you expend?”, not knowing the answer doesn’t mean you don’t understand the <u>concept</u> of “<u>work</u>”, and even if you did know the answer, that would have no bearing on the difficulty of that “<u>work</u>”.  Listening to your brother-in-law’s bad poetry reading that you were forced to sit through could be a lot more “<u>work</u>” than building that stone wall around your garden if you have limited “resources” for listening to bad poetry, but many “resources” for building your stone wall even though it requires 87% less <u>joules</u>/hr.

When we speak of *systems*, we are referring to any process that has identifiable boundaries.  In <u>thermodynamics</u>, a system is simply any matter around which a boundary can be drawn, so a rock is a system, and so is a <u>planet</u> or a galaxy.  Poker is a system because it has <u>conceptual</u> boundaries, such as the rules of the game and the cards used to play.  Cars, living organisms, and computers are systems made up of smaller systems that include things like transmissions, organs, and hard-drives. Even a drawing on a piece of paper can be a system as it has boundaries, and inside those boundaries is something that can be described in terms of <u>energy</u>.

The amount of entropy a system has is related to that system.  For example, a lake and a puddle can both have the same entropy because the entropy does not care about size, only relative ratio.  Imagine you are at the top of a mountain with a tank of water.  Let’s say the tank is holding 100 units of water and has 1 faucet for the water to escape. 

- The water in the tank is low entropy because the water just sits there with nowhere to go, i.e., $\frac{nowhere-to-go}{water}=\frac{0}{100}$=0. 

- When you open the faucet, the water has 1 place to go, $\frac{one-place-to-go}{water}=\frac{1}{100}$=0.01. 

- The water that escapes from the tank disperses itself as it follows the path of least <u>resistance</u> down the mountain, or the path of highest probability of <u>microstates</u> which constantly changes as the water rolls over rocks, soaks into the <u>earth</u>, falls over cliffs, etc., where (in this simple example) will eventually arrive to a river, aquifer, lake, or puddle;$\frac{4-places-to-go}{water}=\frac{4}{100}$=0.04.

- Each destination will receive $\frac{1}{4}$ the original amount of water, or 25 units or water per destination.  The destination then becomes another low entropy state of $\frac{nowhere-to-go}{water}=\frac{0}{25}$=0.

Obviously, this is a massively oversimplified and unrealistic example, but it demonstrates the basic <u>concept</u> of how we are applying the idea of  entropy.

It is the <u>movement</u> of <u>energy</u> from low entropy to high entropy that makes “<u>work</u>” possible.  In this example, we could have placed a small water wheel under the faucet which generated <u>electricity</u> to recharge your flashlight, and this water wheel would have added more entropy to the system.

Now, imagine there are 2 identical water tanks on the top of this mountain, one slightly higher than the other. They have the same entropy, yet the higher tank can do more “<u>work</u>” because if we connected the higher tank to the lower tank, we could move water to power a generator, so it must have lower entropy. How can this 2<sup>nd</sup> tank have 2 states of entropy?

Both tanks can actually have many states of entropy, each one based on the <u>scope</u> of the system.  Both tanks have the same *local* entropy, but they both exist inside another larger closed system, the <u>earth</u>, and within that system they have difference states of entropy due to the difference in gravity due to their relative heights.  This difference gives the higher tank lower entropy because it has more pressure do to the constant and accelerating pull of gravity.  If the higher tank was placed in deep space, far from the gravitational field of the <u>earth</u>, yet sill connected to the lower tank on <u>earth</u>, the water would still move between the space-tank and the earth-tank.  Although there is no gravitational pressure, the earth-tank still has a higher entropy because it is in Earth’s gravitational field, which is equivalent to being 4,000 miles from a 1.7 centimeter <u>black hole</u>, and as mention earlier, <u>black holes</u> have a lot of entropy.  In addition, time slows down closer to a <u>black hole</u>, so earth-tank is in a slower, or lower <u>energy</u> zone of space-time than space-tank, and as <u>energy</u> will always travel the path of least <u>resistance</u>, the water will naturally move to a state of lower-energy.

Does this mean that merely by raising the tank higher we can lower its entropy?  Yes, in the *local* sense, but in the *universal* sense, no, because <u>energy</u> is expended in raising the tank higher, pumping the water higher, or, if filled by rain, the effects of gravity (gravity increases entropy, so the lower tank has higher-entropy-rain falling into it) and velocity (the rain entering the lower tank is moving faster, so disperses more <u>energy</u>, raising the entropy), etc. The increase in entropy of the lower tank offsets the decrease in entropy of the higher tank within the <u>scope</u> of the larger system that contains both local tank systems.

This is also why the common argument that evolution breaks the 2<sup>nd</sup> law of thermodynamics is short-sighted.  The argument is that if entropy always leads to disorder, then how can life continually evolve into more ordered forms if entropy is always increasing?  Either the theory of evolution is wrong, or the 2<sup>nd</sup> law  of thermodynamics is wrong.  The confusion comes from the fact that the 2<sup>nd</sup> law only applies to isolated systems, because, obviously, if a system gets energy pumped into it, the entropy will be lowered.  If the poker deck is fattened by 6 additional <img src="../Images/jack-hearts.png" style="height:15px"/>cards at every shuffle, then, yeah, the 2<sup>nd</sup> law no longer applies to the close system of poker. We can easily test this with a simple capacitor that, when external energy is applied to it, collects positive ions on one side and negative ions on the other&hellip; a virtual impossibility in an isolated system where the 2<sup>nd</sup> law applies.  Likewise, the Earth is *not* an isolated system as it receives 5&times;10<sup>23</sup> HP of energy ever second, which is  3.72×10<sup>26</sup> joules (or watts) per second.  That’s a lot of horses galloping to earth from outer space every second bringing 500,000 times more energy than is consumed by humans.  So, while the 2<sup>nd</sup> law applies to the *closed* system of the universe, it doesn’t apply to Earth’s system which is *open* to the solar system. 

An analogy we’ll often use as a common example of entropy is that of a battery.  A charged battery has low entropy (high usable <u>energy</u>), and a dead battery has high entropy (no usable <u>energy</u>).  A dead battery still has all the <u>energy</u> it had before, but it is no longer usable because the <u>energy</u>, in the form of <u>electrons</u>, has moved from the negative side of the battery to the positive side until it achieved <u>balance</u> and there are no more <u>electrons</u> left on the negative side of the battery that were compelled by the <u>laws</u> of <u>balance</u> (entropy) to move&hellip;  but they are still in the battery, and still acting like <u>electrons</u>, but they are “useless” (from a battery’s <u>work</u> perspective).

Let’s return the the poker hand example.  A junk hand, like<img src="../Images/junk.png" style="height:15px"/>, has high entropy because the cards are disordered, or the order is chaotic. On the other hand, <img src="../Images/flush.png" style="height:15px"/> has low entropy as there is order in both suit and value and there is imbalance as all the cards are close in suit and value, which is very rare.  Its not a coincidence that the higher value of a poker hand, the lower the entropy of that hand.

<img src="../Images/zones.png" style="float:right;width:45%"/>The 2<sup>nd</sup> Law of Thermodynamics states “*In an isolated system, entropy never decreases*”.  This is another way to say that all systems tend to move towards their most stable, or low energy states because entropy, or the *inability* to do work, will always increase.  And why won’t it decrease?  Because, left alone, energy, *on average*, will never imbalance itself.  The “on average” part is important, because, if we wanted to get nit-picky about it, there is an extremely small chance, like 1 in 10<sup>24</sup> , that this law can be broken (see generic probability chart on right), but, technically speaking, you could drop an ice cube in hot water and there is an insanely small chance the ice will get colder and the hot water will get hotter, and it may be the first and last time that will ever happen in existence.  The 2<sup>nd</sup> law is not actually a *law*, but a statistical observation, i.e., there is nothing that says that entropy could not be reversed other than the unlikely probability that it can be reversed.  Even so, there can be a spontaneous decrease of entropy[^369], not to mention the hypothetical and highly speculative case put forth that the 2<sup>nd</sup> law exists in reverse somewhere in the Universe[^370], thereby keeping the entropy level of the Universe at 0. It may be easy to simply toss this idea aside until you consider it was written by the man who is considered the smartest human to have ever lived (that we know about.  See *Appendix K, “William James Sedis”*).

[^369]: Xing Xiu-San, “”Spontaneous entropy decrease and its statistical formula”, Department of Physics,Beijing Institute of Technology,Beijing ,China; https://arxiv.org/pdf/0710.4624.pdf
[^370]: Sidis, W.J. (2011). The Animate and the Inanimate.  Originally published in 1920. https://www.sidis.net/animate.pdf

We can apply the <u>concept</u> of entropy to any 2 things that are different and interact with each other.  For example, have you ever wondered why one drop of black paint in a can of white paint makes a big difference, but one drop of white paint in a can of black paint makes little difference?  We can say that white paint has high entropy and black paint has low entropy if we defined the <u>concept</u> of “<u>work</u>” not as <u>kinetic</u> <u>energy</u> but as how much light was reflected or absorbed by a color.  We could measure how much “<u>work</u>” each color performs.  If white paint has an entropy of 1 (absorbs least) and black paint an entropy of 10 (absorbs most), we would discover that if we mix 1 part to 10 of each than the white color changes by 10%, but the black color changes by only 1%.  This application of entropy may <u>work</u>, but it is also confusing because now we are using the same terms and <u>concepts</u> that equates the color black with dispersion, disorder, <u>balanced</u> <u>energy</u> and the color white with pressure, <u>pattern</u>, <u>order</u>, and imbalance.  This is a good example as to the importance of context.  Entropy can be measured as <u>energy</u> <u>radiation</u> in one context, but <u>energy</u> absorption in another.

The universality of the <u>concept</u> of entropy is why it appears in everything from <u>thermodynamics</u> to culture to economics, and anything else that can have <u>microstates</u>, be they chemical, electrical, physical, spiritual, emotional, <u>conceptual</u>, etc..  For example:

-   **In probability theory**, the entropy of a random variable is the measure of uncertainty.
-   **In <u>information</u> theory**, the *compression entropy* of a compressed file, like a zipped file or a JPEG image, measures the amount of <u>information</u> loss.
-   **In sociology**, entropy is the natural decay of a <u>society</u>’s <u>structure</u> (such as <u>law</u>, organization, convention, ethics, etc.).   This also applies to cultures.  Think of the Green Berets of the U.S. Army Special Forces as having low entropy (high ability to <u>work</u>) , and a 1967 free-love hippie commune in Haight-Ashbury as high entropy (low ability to <u>work</u>).  *Note: I have nothing against free-love hippie communes. I have lived on a few and even resided in Haight-Ashbury back in the day.*

We may be taking liberties with the <u>concept</u> of entropy, but even in the <u>science</u> world the definitions of entropy are so diverse and specific that even scientists get confused:

> “As a consequence of this diversity of uses and <u>concepts</u> [of entropy], we may ask whether the use of the term entropy has any meaning.  Is there really something linking this diversity, or is the use of the same term with so many meanings just misleading?”[^13].  **~<u>Annick Lesne</u>, author and researcher at the Institut des Hautes Etudes Scientifiques**

[^13]: Lesne, A.  (2014).  **Shannon entropy: A rigorous notion at the crossroads between probability, information theory, dynamical systems and statistical physics**.  *Mathematical Structures in Computer Science,* *24*(3).  doi:10.1017/s0960129512000783

<center><IMG src="../Images/entropy-trigram.png" style="width:70%"/></center>

For our purposes, we can accept that entropy is not so much a measure of <u>chaos</u>, but a *dimension of <u>chaos</u>*, but where *<u>chaos</u>*, in general, is defined as the *“lack of <u>order</u> in form or <u>movement</u> of <u>energy</u> due to the dispersion and <u>balancing</u> of <u>energy</u>*”.  

For this <u>reason</u>, we defined 2 types of <u>chaos</u>; *high-entropy <u>chaos</u>* and *low-entropy <u>chaos</u>*.  This is the same <u>concept</u> as the *<u>chaos</u> of 0* and the *<u>chaos</u> of &infin;* as mentioned in the first chapter, but here the context is the material reality, not mathematics.

#### **Key 22:** Just as there is the *<u>chaos</u> of 0* and the *<u>chaos</u> of* &infin;, there is *low-entropy <u>chaos</u>* and *high-entropy <u>chaos</u>*.

The image above is a general diagram of this <u>concept</u>.  The 3 <u>patterns</u> in the center show examples of low-entropy (right), high-entropy (left), and a <u>balance</u> of low and high entropy (center).  It’s not a coincident that the center image looks a bit like a plant, as we show later that reality, and living things especially, are a mixture of of <u>order</u> and <u>chaos</u>, high and low entropy combined.  This may be due to the fact that <u>evolution</u> of complex systems actually decreases entropy, while the process of growth itself increases entropy, making <u>life</u> a constant <u>negotiation</u> between these 2 opposite forces.

We can also see that:

- Everything moves from low to high entropy as a result of some sort of dynamic process that dissipates, such as heat in the case of <u>thermodynamics</u>.

- Everything exists on the spectrum of low&rarr;high entropy

- The effect is a result of the cause which becomes a new cause for a new effect.  This is like the cause/effect chain of <u>microstates</u> within <u>microstates</u>; shuffling the deck will result in 1 of 8.06<sup>67</sup>  <u>microstates</u>, and each of those <u>microstates</u> (the hand that was dealt) has 2.6 million <u>microstates</u>. If a hand is part of a favorable macrostate, new <u>microstates</u> will emerge when the holder puts the hand into play.  Another example is how ice is the result of a previous cause/effect chain.  This ice now has it’s own <u>microstates</u>, or *entropic path*, starting out as ice and ending up as vapor, some parts of which ending up in a cloud which will have a different cause/effect path.  Along that path it will be a liquid which will have its own cause/effect path as well, perhaps with its relationship to some salt or sugar, or as hot coffee that gets cold.

<img src='../Images/L-sys-2.png' style='float:right;width:20%'/>Microstates within microstates within microstates… this is equivalent to the previous self-similar fractal pattern of the rainbow bush, but unlike our rainbow bush, the entropic systems branching off one another are chaotic, dynamic systems, so not simply chaos, but an endless chain of chaotic systems within chaotic systems.  But is every system with microstates chaotic?  To some degree, yes, but where the predictability is near 100% it doesn’t *appear* chaotic.  Perhaps we can say it’s only a “little bit” chaotic.  But even the most predictable event only exists at the end of a long chain of chaotic, (previously) unpredictable events.  Consider all the things that had to take place from the first cause to make a predictable coin-toss possible… planets had to be formed, life had to evolve, consciousness and meta-consciousness had to evolve and be “discovered”, etc.  The coin-toss may not be very chaotic, but countless events leading up to it certainly were, which is why everything is part of the chaotic system that is this reality.  

If everything is chaotic, where is the <u>order</u>?  Ironically, <u>order</u> is a subset of <u>chaos</u>, and everything *is* unpredictable to some degree, and that “degree” is measured in *probability*.[^334]  We live our lives as if the sun rising tomorrow is 100% guaranteed, but it isn't.  Sure, there may be only a 1 in 10<sup>-99</sup> chance it won’t rise, but it’s still a probabilistic value.  And the <u>reason</u> this probability is so high is because the solar system has reached a state of equilibrium, a state of high entropy.  Not its highest state of entropy, for when that happens, everything is reduced to cosmic dust and/or <u>black holes</u>, but enough equilibrium to make it <u>sustainable</u>&hellip; for now.  There simply aren’t any more <u>microstates</u> that are more likely than the one we currently live in.  If this equilibrium is disturbed by a huge foreign object passing through our space or a change in temperature of the sun or some other change large enough to create more <u>microstates</u> than currently exist, then the <u>chaos</u> of low-entropy/high-energy will ensue and rearrange everything.  Order emerges when some degree of equilibrium of the parent system is reached.  As the parent system slowly increases its entropy and looses <u>energy</u>, the more energetic, more chaotic, low-entropy child system becomes more stable until it also reaches equilibrium as it loses <u>energy</u>.  Life on <u>earth</u> could not begin until the <u>chaos</u> of Earth’s early days settled down; Earth could not form until the Sun was finished forming in the center of a nebula that was formed by gravity’s effect on dust and gas, etc., etc., all the way back to the <u>Big Bang</u>.  Not one single thing, system, or <u>order</u> exists that is not dependent on the equilibrium of the system it is built upon.  

[^334]: Strevens, Michael. **Bigger than Chaos: Understanding Complexity through Probability**. London: Harvard University Press, 2006. 

Does this mean that one day the probabilities of getting heads in a coin flip will not be 50%?  In one sense yes, because when all matter and <u>energy</u> is equally dispersed, there won’t be any coins, <u>humans</u>, or <u>planets</u>, so while the <u>archetype</u> of that system might exist, the reality of it will not.  It must be stated that this is according to the traditional *Heat Death* theory of how the universe ends.  There are other theories, but in any case, the stable systems will destabilize, and no one will exist to toss coins that don’t exist.  The coin-toss and the coin-tossers both exist because the system of our current reality, the state of reality at this moment in the life-cycle of the Universe, allows for them to exist.  Imagine tossing a coin during the <u>Big Bang</u>, or inside a <u>black hole</u>.  It’s a silly idea because all the <u>laws</u> that allow for tossing and tossers, while they still exist, are uninstantiated and exist only as <u>archetypes</u>.

In our current system, a <u>coin toss</u> is not a random event but one of <u>deterministic</u> <u>chaos</u>.  The <u>reason</u> it is unpredictable is because there are too many micro variables in a toss that effect the outcome; muscle <u>movement</u>, atmosphere, relative starting position, spin axis, initial velocity, imperfections in the coin, whether it’s to be caught or allowed to bounce around when it lands, etc.  A common coin flip, where the coin is not allowed to bounce around when it lands, is a 12 dimensional system, as opposed to the 2-dimensional system of its <u>archetype</u>. This means that while there are only 2 possible outcomes of heads or tails, with 12 dimensions there are 479,001,600 ways to get there.   If we could flip a perfectly vertically <u>balanced</u> coin (because a horizontal coin has a 51% chance of landing in the same orientation as it started because there will always be 1 more even <u>number</u> of flips) in a vacuum with precise pressure on a precise location, and do it identically each time, the result could easily be predicted by Newton’s Law of Motion and angular momentum, at least in theory.  The possibility of creating such a system is (probably) physically impossible, thanks to <u>chaos</u>.  Because the system of a <u>coin toss</u> is a subsystem of countless parent subsystems, not only those of time and space, but those of the tosser, the environment, the coin, etc., all of which are descendants from the initial <u>chaos</u> of creation, any change to any of those parent systems will change the <u>coin toss</u> system.  

Of course, in the quantum world, the coin flip is like a Schrödinger’s Cat scenario, but instead of the cat being in an *eigen state* of dead *and* alive, the unobserved coin is in a eigen state of heads *and* tails, and it is only the observation of the coin that determines which state will collapse into reality.  In this world view, everything that is probabilistic (which is everything that happens), is undetermined until we observe it.  However, as the results of such quantum events results in the same outcomes as the old fashion probabilities and classical physics, we’ll stick to that. [^335]

[^335]: Albrecht, Andreas, and Daniel Phillips. “**Origin of Probabilities and Their Application to the Multiverse.**” *Physical Review D* 90, no. 12 (2014). https://doi.org/10.1103/physrevd.90.123514.  https://arxiv.org/pdf/1212.0953

The <u>laws</u> we live in are, for now, stable enough, but they may be changing as you are reading this.  We know, at least according to our current <u>understanding</u>, that even the most basic <u>laws</u>, like Newton’s Laws of Motion, will completely break down as the universe approaches *statistical equilibrium*, which means there is an equal probability that <u>energy</u> will move in any direction in a reality where all matter, which has decomposed into <u>atomic</u> dust, just wiggles around like 10<sup>86</sup> lost atomic-sized bugs in space as they slowly wait to get sucked into a <u>black hole</u>.  The <u>speed of light</u>, Planck’s Constant, and every other constant, may be changing due to the expansion of the universe, quantum changes, increasing <u>dark-matter</u>, and who knows how many other things we haven’t even discovered yet.  Many scientists have not only theorized this, but there is some evidence to support this idea.  The problem is, on a cosmic time scale (using the new theory that the universe will come to an end in a mere 4 billion years[^333]),  if the universe lasted 100 years, we <u>humans</u> have only been measuring things for the equivalent of 43 millionths of a second (5,000 years), so we can only speculate about the other 99.9999995% of <u>existence</u>.

[^333]: Bousso, Raphael, Ben Freivogel, Stefan Leichenauer, and Vladimir Rosenhaus. “**Eternal Inflation Predicts That Time Will End.**” *Physical Review D* 83, no. 2 (2011). https://doi.org/10.1103/physrevd.83.023525. https://arxiv.org/pdf/1009.4698v1.pdf

#### **Key 23:** Order is high-entropy <u>chaos</u> that exists within high-entropy <u>chaos</u>.

Entropy tells us how close a system is to equilibrium, where perfect equilibrium equals perfect disorder, (because all the parts are spread out equally).  Random dots on a page is perfect equilibrium, and therefore high entropy. If those dots create any sort of <u>pattern</u>, then this is not an equal distribution of dots, and therefore has lower entropy, which means there must be more <u>energy</u> available for “<u>work</u>”.  Exactly what “<u>work</u>” means with regard to <u>information</u> we won’t get into here, but as proof of this, below are the results of the entropy analysis[^332] of 4 images, each with 64,000 black dots on a white background.  You can easily see that more <u>order</u> equals less entropy.

[^332]: Software used: Arch Linux, DiE (detect-it-easy) v3.03, Nov 14, 1021

<center><IMG src="../Images/ent-images.png" style="width:100%"/></center>

So, where does <u>chaos</u> fit into this?

<IMG src="../Images/entropy-hilo.png" style="float:right;width:40%"/>Earlier, we defined 2 kinds of <u>chaos</u>; *high-entropy <u>chaos</u>* and *low-entry <u>chaos</u>*.  We also posited that the most efficient and stable expressions of <u>energy</u> will exist in the middle of these poles.   This naturally divides the spectrum into 2 parts; *ascending <u>order</u>* and *descending <u>order</u>*.  This is shown in the image on the right using our previous model (and flipping the X-axis to match the traditional left &rarr; right x-axis format).  As this is a spectrum of <u>chaos</u>, the *ascending side* that shows a positive growth of <u>order</u> will be called *positive <u>chaos</u>*, and the descending side, which shows negative growth of <u>order</u> is called *negative <u>chaos</u>*.

- **Positive Chaos** (+<u>chaos</u>, low-entropy <u>chaos</u>): The <u>chaos</u> that begins with the single point that represents the totality of <u>somethingness</u> and from there expands.  On a universal scale, this would include the moment of the <u>Big Bang</u> to the moment of maximum <u>order</u>.  On the scale of <u>life</u>, this would represent the time from a germinated <u>seed</u> to a flower.  This describes the *<u>implicate</u> &rarr;<u>explicate</u>* stage of creation. 
- **Negative Chaos** (-chaos, high-entropy <u>chaos</u>): The <u>chaos</u> that begins when the maximum expression of <u>order</u> begins to decline and continues until all <u>order</u> has been disintegrated. Universally, this equates to a dead system, no <u>pattern</u>, no <u>movement</u> of <u>energy</u>, no <u>order</u> of time… just a bunch of dead matter, totally diffused, perfectly <u>balanced</u>, doing nothing.  On a scale of <u>life</u>, it is the journey from flower to compost.  This describes the *<u>explicate</u> &rarr;<u>implicate</u>* stage of creation. 

It would seem more symmetrical and intuitive if the *+<u>chaos</u>* and *-chaos* moved in opposite directions, but that would mean that the *-chaos* would be going backwards in time, which is impossible as that would completely break the 2<sup>nd</sup> Law of Thermodynamics… right?  Not according to the man who has been called “The most intelligent man to ever walk the Earth”.  That might be a bit hyperbolic, and certainly impossible to prove as IQ tests, as we know them today, did exist in his time, but William James Sidis, who spoke 8 languages, wrote 4 academic books by age 8 and was reading the New York Time at 18 months, is alleged  to have had an estimated IQ of 250-300, which is ironic as he considered any sort of <u>intelligence</u> testing as “silly, pedantic, and grossly misleading”.  Still, to put this in perspective, in the entire world today, there is a statistical chance that 1 person on the <u>planet</u> has an IQ of 192.  An IQ of 200 is a 1:76,000,000,000 rarity, so with an IQ of 250+, statistically speaking, he may well have been the smartest human to ever exist (See *Appendix K, “William James Sidis”*, for more)

<IMG src="../Images/entropy-hilo-2.png" style="float:right;width:50%"/>According to Sidis’ highly speculative hypothesis, “*The Animate and the Inanimate*”[^389], published in 1920 when he was 20 years old, entropy could, in theory at least, be reversed, or rather, there exists parts of the Universe that run opposite to ours, like a mirror image of this reality.  These “parts” are interspersed with the “parts” that run “forward”, similar to a checkerboard, but we will never be able to see them or travel to them because they exist is a different space-time.  This sounds like a very early <u>concept</u> of the modern “many worlds” hypothesis of <u>quantum mechanics</u> and is share many of the same <u>concepts</u> of the modern idea that before the <u>Big Bang</u>, the Universe was a reflections of what it is today[^336] .  In this reversed universe, time does not run backwards in the way you might think because its *anti-time* or *negative-time* properties exist in a universe of *anti-matter*, so everything appears the same to those living in *anti-world* as long as all matter, <u>energy</u>, and time were equally inverted.  This is called the *charge, parity, and time (CPT) reversal symmetry*, and it is well understood as *The C.P.T. Theorem*.  If we woke up tomorrow morning in the *anti-world* version of our current reality we live in now, we would not notice anything different, as long as the CPT was inverted.   Applying Sidis’ <u>ideas</u> to our model might look something like the right image, which represents just 2 “squares” on the checkerboard of reversing realities, but this image would also apply to the modern CPT <u>ideas</u>, but where the 0 low-entropy (green) dot would represent the <u>Big Bang</u>.

[^389]:Sidis, W.J. (2011). The Animate and the Inanimate.  Originally published in 1920. https://www.sidis.net/animate.pdf
[^336]: Boyle, Latham, Kieran Finn, and Neil Turok. “The Big Bang, CPT, and Neutrino Dark Matter.” *Annals of Physics* 438 (2022): 168767. https://doi.org/10.1016/j.aop.2022.168767, https://arxiv.org/pdf/1803.08930.pdf

Being of moderate IQ and abilities, I can’t say anything as to the validity or insanity of Sidis’ <u>ideas</u>, but given that Sedis predicted <u>black holes</u>, the expanding universe, and the <u>Big Bang</u> using only the 2<sup>nd</sup> Law of Thermodynamics, and years before the discovery of the expanding universe and the <u>Big Bang</u>, the odds are in his favor.  Regardless, it is not the validity of this idea that is important here, it is the <u>patterns</u> that this idea implies, which is a <u>pattern</u> fundamental to all of <u>existence</u>, from ancient oracles to <u>electricity</u> to <u>DNA</u>, as we’ll see.

As this process of emergence and decay of <u>order</u> applies to any system, then it also applies to not just the <u>life</u> of the Universe, but the <u>life</u> of all the systems created in the Universe, and the systems that those systems create, etc., etc., making reality a dynamic <u>fractal</u> of countless embedded systems of  *low entropy <u>chaos</u> &rarr; <u>order</u>&rarr; high entropy <u>chaos</u>*. 

<center><IMG src="../Images/chaos-chain.png" style="width:70%"/></center>

In this manner, the emergence of an apple follows the same rules as the emergence of the Universe, as does the tree, weather, <u>planet</u>, solar system, etc., that made the <u>existence</u> of that apple possible.  The <u>energy</u> and the <u>archetypes</u> of these rules instantiate within the context and <u>scope</u> of each creation which we recognize as the <u>laws</u> of creation for each context and <u>scope</u>. The example (right image) is a very simple model, but we are following the advice of the award winning quantum physicist Philippe Nozières:

> “Only simple qualitative arguments can reveal the fundamental [<u>laws</u>]”

Because this dispersion of <u>energy</u> is the result of <u>energy</u> always seeking <u>balance</u>, we say that entropy is a measure of the state of <u>balance</u>, as in higher entropy equals more <u>balance</u>, or equilibrium of <u>energy</u>.

#### **Key 24:** Entropy is a measure of <u>balance</u>.

If this is true, then it is also true that:

#### **Key 25:** “Work” is the act of <u>balancing</u> <u>energy</u> and is a result of the imbalance of <u>energy</u>.

“Entropy” is a noun as it only described the state of a system.  What, then, is the verb for the actions that lead to that state?  If entropy is a measure of <u>balance</u> and <u>work</u> is the act of <u>balancing</u>, then “<u>work</u>” is the verb form of entropy.  More <u>work</u> = more <u>balance</u> = more entropy.

Because we will be talking about <u>balance</u> throughout this entire book, let’s make it clear what we mean when we use the word.  “Entropy” and “<u>work</u>” are different words to describe different, yet interdependent <u>concepts</u>, but the single word “<u>balance</u>” is synonymous with both <u>concepts</u> because the word “<u>balance</u>” can be a verb *and* a noun:

- **Balance:** (verb) The application of <u>force</u> required <u>to achieve a stable state of being</u>; Work.
- **Balance:** (noun) The achieved stable state of being; Entropy.

The word itself comes from the Latin *bi+lanx*, meaning “two sauce pans”, as in the classic hanging scales.  The scales show when something is *<u>balanced</u>* (noun) after the two sides were *<u>balanced</u>* (verb).  It will be up to the reader to determine which <u>definition</u> is more contextually appropriate.

Before we leave the subject of entropy, there is another <u>concept</u> to keep in mind that will come up again later:

**<u>Information is entropic</u>.**  This <u>concept</u> was demonstrated in a thought experiment of James Maxwell in 1867 that threatened to break the 2<sup>nd</sup> Law of Thermodynamics.  The gist of this hypothetical was if a magical being of some sort that generated no heat or friction was able to calculate the trajectories of every <u>atom</u> in two chambers, one filled with hot gas and one filled with cold gas, he would be able to selectively allow certain <u>atoms</u> to pass through a door that he could open or close that would make the hot side hotter and the cold side colder.  This was a massive violation of the 2<sup>nd</sup> <u>law</u>, and the inability for anyone to figure out the solution was why Lord Kelvin dubbed this imaginary being *Maxwell’s Demon*.  100 years later, the solution was found: the <u>energy</u> needed for this demon to collect and store all the <u>information</u> on all the <u>atoms</u> would generate enough entropy to more than compensate for the seeming impossible result.  Hence, <u>information</u> itself has entropy, or more correctly, <u>information</u> that is stored.  That storage could be a hard drive, scribbled notes, the <u>brain</u>, or whatever matter the <u>information</u> is stored in/on.  It was also shown that erasing that <u>information</u> raised the entropy even more (in case you were thinking of tricking entropy by only storing a minimum amount of new <u>data</u> and constantly erasing the old <u>data</u>).

#### **Key 26:** Order and <u>pattern</u> requires <u>energy</u>.

The inter-connectivity of things, whether directly or via radiations such as heat and light, allows for <u>movement</u> of <u>energy</u> and this <u>movement</u> creates <u>order</u>.  Therefore, the idea that everything is connected is not simply a philosophical abstract <u>concept</u>, but a necessity for <u>order</u> and therefore <u>life</u>.

#### **Key 27:** Everything is in a state of seeking and/or maintaining <u>balance</u>.

At first, the idea that perfect <u>balance</u> results in perfect <u>chaos</u> seems contradictory, or at least counter-intuitive probably because in our <u>life</u>, and in reality in general, (near) perfect <u>balance</u> only exists in small systems, only temporarily, and under very controlled conditions.  When it does exist, we tend to ignore it because perfect <u>balance</u> doesn't *do* anything, as there is no <u>movement</u>, no <u>energy</u>.  Outside of a laboratory, something that is perfectly <u>balanced</u> is more likely to be tossed in the garbage.  However, if we can extend the idea of <u>balance</u> beyond the classical <u>ideas</u> of <u>movement</u>, we can understand this <u>concept</u> better.  

We know that all of <u>existence</u> is a combination of *systems* within *systems*, from the system of galaxies to the systems of plants and pebbles.  This applies to other contexts as well, such as economics.  When we pay $3 for a pound of carrots we have <u>balanced</u> the difference between a particular instance of supply (of that bag of carrots) and demand (for Bob’s carrot soup).  Once that transaction is complete, there is perfect <u>balance</u> in the very small *system* of that transaction.  That particular instance, being perfectly <u>balanced</u>, no longer has any <u>energy</u> moving through it.  It is a dead systems, and is relegated back to the <u>chaos</u> from which is came.  That instance was but one of many in the larger systems that had to exist for it to even instantiate in the first place; markets, sales, distribution, supply chains, farming, etc., etc., and all the systems they depend on as well to exist. 

We know that, ultimately, these systems exist for a single purpose, the <u>balancing</u> of <u>energy</u>.  We also know that the more <u>microstates</u> there are, the more opportunity there exists for more <u>energy</u> to <u>balance</u> quicker.  This means that the ultimate goal of <u>energy</u>, which is <u>balance</u>, can be enhanced, or optimized, or made more efficient, if there are more <u>microstates</u>, and because it is more efficient, the <u>movement</u> of <u>energy</u> will naturally find a way to create more <u>microstates</u>.  And how will <u>energy</u> create conditions where more <u>microstates</u> will naturally emerge? Through <u>order</u>! Consider the tremendous amount of <u>order</u> on countless levels that must be maintained for you to buy a &dollar;3 bag of carrots from a farm 3000 miles away.

#### **Key 28:** The purpose of <u>order</u> is to facilitate <u>chaos</u>.

Economics, and anything else that we create, <u>conceptually</u> or physically, is as natural a form of <u>evolution</u> as lifeless bio-goo evolving into <u>life</u>, and for the exact same <u>reason</u>.

In the old days, 3.8 billion years ago, when there was plenty of blobs of carbon-based bio-goo, but before <u>life</u> existed, the bio-goo blobs were getting a lot of this free-energy from the sun, the very hot molten core, and the very hot water.  According to the <u>laws</u> of <u>energy</u>, these blobs needed to expend the same amount of <u>energy</u> they took in, but carbon-based blobs have no special energy-dissipation abilities, at least not as blobs.  Energy moving through the blobs, or anything fro that matter, will consequently cause changes that allow for more efficient <u>movement</u> of <u>energy</u>.  Over time, the blobs naturally reorganized to allow better dissipation of excess <u>energy</u>.  Patterns of <u>energy</u> began to form in these blobs, and these <u>patterns</u> became what we have come to know as <u>life</u>.  This is (perhaps) why the earliest forms of <u>life</u> are believed to be 4+ billion year old microorganisms that formed on the hydro-thermal vents on the ocean floor that spew out boiling hot water up to 500&deg;C (930&deg;F).  That heat was their source of <u>energy</u>, and it delivered significantly more <u>energy</u> than the sun (and continues to be a significant <u>contributor</u> to ocean warming as well as methane gas).  The earliest direct evidence of <u>life</u> are the 3.5 billion-year-old microfossils found on the hot water vents in the Pilbara region of Western Australia.

According to Jeremy England, a 31-year-old assistant professor at the Massachusetts Institute of Technology who derived a mathematical formula[^374]that explains how the <u>movement</u> of <u>energy</u> can convert a carbon-based blob into a <u>life</u> form:

>  You start with a random clump of <u>atoms</u>, and if you shine light on it for long enough, it should not be so surprising that you get a plant.

But it’s not just for plants, as we see this also in non-carbon blobs, for example, self-assembling <u>crystals</u> and quasi-crystals.

#### **Key 29:** Life is a natural consequence of the <u>movement</u> of <u>energy</u> for the purpose of moving <u>energy</u>.

[^374]: Jeremy L. England , “”**Statistical physics of self-replication**”, J. Chem. Phys. 139, 121923 (2013) https://doi.org/10.1063/1.4818538

In that first moment the Universe came into being there was the total <u>energy</u> of the Universe in the space of a soccer ball that was released into a void of <u>nothingness</u>.  It was the maximum state of imbalance that began the <u>balancing</u> act of reality. At the end of the Universe, there is no longer any need for <u>energy</u> to <u>balance</u> itself because everything has become <u>balanced</u>. The Universe started with a verb, and ends with a noun.  Wherever there is <u>energy</u>, there is some level of <u>balance</u>, and we say that things are “<u>balanced</u>” when the net <u>energy</u> difference between any two states is 0.  Prior to that difference being 0, there is *<u>balancing</u>*, the <u>movement</u> of <u>energy</u>, and this <u>movement</u> creates <u>order</u>.

#### **Key 30:** Chaos is a state of <u>order</u>, <u>order</u> is a state of <u>chaos</u>.


## Inertia

Another <u>law</u> to look at is Newton’s 1<sup>st</sup> Law of Motion, the Law of <u>Inertia</u>, which states *an object will remain at rest or move at a constant speed in a straight line unless it is acted upon by an unbalanced <u>force</u>*.  This is why things don’t fly around randomly on their own for no <u>reason</u>.  This is mentioned here because it is important to remember that nothing moves, nothing does anything, unless <u>energy</u> is applied as <u>force</u>.  <u>Inertia</u> is to <u>movement</u> what gravity is to things.  Gravity and inertia are two aspects of the same thing as you can calculate the gravitational field of something by how much <u>energy</u> is needed to move it.

These are just two of the many <u>laws</u> that determine how our reality works, at least within the <u>scope</u> of the reality we tend to deal with.  On the quantum and galactic levels, or super high or low <u>energy</u> states, things may operate a bit differently.

Inertia and entropy always ensure that everything will operate at its most efficient level.  What does “operate” mean in this context? It means the optimal <u>movement</u> of <u>energy</u>.  And what does “optimal” mean? It means the most efficient way to <u>balance</u> <u>energy</u>.  As <u>energy</u> only moves when there is a difference between two states, the <u>movement</u> of <u>energy</u> is meant to do only one thing, and that is to minimize that difference by creating a <u>balance</u> between two conditions that are not the same, whether it’s the conditions of *<u>somethingness</u>* and *<u>nothingness</u>* or just a few degrees of temperature.  Once <u>balance</u> is achieved, the <u>movement</u> stops.  A <u>balanced</u> battery is a battery at peace with itself&hellip; and it is also a dead battery.

The optimum condition for the <u>movement</u> of <u>energy</u> between two states is one where both states have maximum expression within the <u>limitations</u> and abilities of both of those states.

<img src='../Images/008-harmonograph.png' style='float:right;width:50%'/>A beautiful example of entropy and inertia that works today as much as it did thousands of years ago can be easily demonstrated with something called the *harmonograph*.  This is a fascinating and entertaining device that creates an oscillation from an initial push (low entropy, high imbalance) and then draws a trace of its path as that initial energy slowly diminishes (inertia, balancing), until it stops (high entropy, balanced).[^16]

[^16]: Here are a couple of sites that show a harmonograph in action.  They are fascinating to watch.  <http://andygiger.com/science/harmonograph/index.html>, <https://www.youtube.com/watch?v=HJYvc-ISrf8>

## Pattern

We recognize that <u>patterns</u> exist in <u>nature</u>, <u>life</u>, physics, <u>math</u>, etc., and it can be useful to think of <u>patterns</u> like rivers that have been etched into the terrain of reality from the first moments of creation and represent a path of least <u>resistance</u> for the <u>movement</u> of <u>energy</u>, be it electrical, physical, <u>conceptual</u>, emotional or otherwise.  Reality, no matter the <u>scope</u> or how one perceives it, is build upon layers and layers of <u>patterns</u> that sit on top of the <u>chaos</u> from which they emerged.  A recurring theme in this book is about recognizing <u>patterns</u> that repeat in various themes, contexts, and scopes.  Some of these meta-patterns are clear, such as the Fibonacci sequence and exponential curves, while others are more hidden and even challenges our <u>ideas</u> of what defines a <u>pattern</u>.  A nice book on this subject is “*Patterns in <u>nature</u>: Why the natural world looks the way it does*” [^17].

[^17]: Ball, P.  (2016).  **Patterns in nature: Why the natural world looks the way it does**.  Chicago: The University of Chicago Press.  ISBN-10: 022633242X ISBN-13: 978-0226332420

The position put forth here is that any two phenomena that share the same <u>pattern</u> are, at least, two instances of one <u>pattern</u>. This differs from the scientific view that the <u>pattern</u> is a result of the cause, and while this makes sense, it also makes sense that it is the <u>archetype</u> of a <u>pattern</u> that is the cause of the instance (form).  It’s really a matter of perspective.

 A Ford Fairlane in Chicago and a Ford Fairlane in Argentina have no direct connection to one another, but some of what is learned about one will apply to the other.  The tricky part is knowing what <u>information</u> is rooted in the <u>patterns</u> versus the instance and its context.  For example, the Ford Fairlane was a “muscle car” and a source of pride among its fans in the U.S.  That same car in Argentina was used by the secret police of the Dictatorship during the “Dirty War” and was a source of fear and intimidation, but an Argentine mechanic could <u>work</u> on that same car in Chicago, even it seeing it filled him with dread.  

While the subjects of this comparison can be defined by <u>patterns</u> of culture, economics, mechanics, and more, the erroneous associations comes from assigning properties of one context’s <u>pattern</u> to another.  This results in superstition and it close cousin, dogma. 

**Superstition**: A belief for which there appears to be no rational substance.

**Dogma**: A belief that people are expected to accept without any doubts.

The sciences are not immune to such confusion.  In fact, modern <u>math</u> was stifled for about a thousand years thanks to the Greek’s superstitious <u>ideas</u> about, and prohibition of, irrational <u>numbers</u>, which were forbidden to be written.  When one of Pythagoras’ students, Hippasus, discovered irrational <u>numbers</u>, he was (depending on your source of history) drowned by the gods for such heresy, or murdered because he revealed how to construct a dodecahedron inside a sphere.  In either case, irrationality and superstition prevailed.  It wasn’t until the early 9<sup>th</sup> century –CE, thanks to Caliph Al-Ma’mun of the Abbasid Caliphate and his *House of Wisdom* in Baghdad (which the Mongols destroyed in the 13<sup>th</sup> century), that mathematics was liberated from its fear of irrational <u>numbers</u>.  We saw earlier that same was true for the <u>number</u> 0, which was banned in Florence in 1299.  Later in this book you will see modern examples of similar thinking in the sciences.

## Oscillation

A <u>law</u>, or <u>pattern</u>, that can be seen in every part of the universe, is that everything oscillates in some manner.  As one of the many definitions of <u>chaos</u> is “*<u>chaos</u> is a kind of <u>order</u> without periodicity*”, there can be chaotic <u>movement</u> of <u>energy</u>, but <u>oscillation</u> allows for a more <u>sustainable</u> <u>movement</u> of <u>energy</u>.  This is why just so many things that exist, from the <u>atoms</u> to the galaxies, oscillate in some fashion and has some kind of <u>frequency</u>.  According to Tesla, this applies to all things that exist: 

> “All things have a <u>frequency</u> and a vibration.”  **\~<u>Nikola Tesla</u>**

#### **Key 31:** Everything exists in a state of duality.

Because everything oscillates 

<img src='../Images/spirals.png' style='float:right;width:50%'/>We typically think of light waves and sound waves as the classical example of oscillations, but the heavenly bodies are also oscillating particles on a cosmic scale.  If we look at the orbits of planets, stars and galaxies they are not simply spinning around in a rather 2D plane of orbits, but that they are spinning around while moving in a direction.

People have been fascinated with this obvious commonality across all of creation for some time.  <u>Kepler</u> himself was quite interested in the relationship between <u>planetary</u> <u>frequencies</u> and musical <u>frequencies</u>, but the study of <u>planetary</u> and musical relationships goes back to at least the 9th century with Eriugena, an Irish monk, theologian, and Neoplatonist philosopher, most famous for his <u>work</u> “The Division of Nature”, which claims that <u>nature</u>’s first primary division was the division between that which **is** (*being* or *<u>somethingness</u>*) and that which is **not** (*nonbeing* or *<u>nothingness</u>*).  His <u>work</u> was condemned as “*swarming with worms of heretical perversity*”.  The 9<sup>th</sup>-century Archdiocese was a tough crowd.

Energy oscillates, and as matter *is* <u>energy</u>, matter also oscillates.  The <u>electrons</u>, protons, and nuclei that constitute all matter are themselves *systems* of oscillating <u>energy</u> fields  When these various *systems* come together to form <u>sustainable</u> <u>patterns</u> of oscillations, they create a new *system*, the <u>atomic</u> <u>structure</u>, which is a high-frequency oscillating <u>energy</u> grid.  For example, a *system* of the nucleus oscillates at ~10<sup>22</sup> Hz.  The *system* of an <u>atom</u> (at 70 &deg;F) oscillates at ~10<sup>15</sup> Hz. An entire molecule, which is a *system* made of *systems* made of *systems*, oscillates at ~10<sup>9</sup> Hz, and so on[^373]. The key ingredient as to what makes a *system* is the sustainability of the oscillations, as that will determine how it integrates into its context and environment.  More significantly, it is the oscillations that are fundamental in the <u>definition</u> of a *system*, as its oscillations define how it will interact with other systems.

[^373]: Source of frequency values: Bentov, Itzhak. “**Stalking the Wild Pendulum: On the Mechanics of Consciousness**”. New York: Bantam Books, 1979. Print.

#### **Key 32:** <u>Oscillation enhances sustainability</u>.

<center><img src='../Images/rotation1.png' style='width:80%'/></center>

*(Image from Grant Sanderson’s video “But what is a Fourier series? From heat flow to circle drawings”*. [^305]

[^305]: at https://www.3blue1brown.com/

On a more abstract level, everything that we can see, hear and touch can be described as a collection of oscillations.  This was proven by the famous <u>French mathematician</u> <u>Joseph Fourier</u> (1768–1830) when he was researching how heat moves.  From his <u>work</u> came the famous and brilliant *Fourier Series* and *Fourier Transformation*.  The Fourier Series describes the <u>hierarchical</u> <u>order</u> of <u>frequencies</u> that would be needed to produce a specific output.  For example, the image above shows a portrait of <u>Joseph Fourier</u> being drawn by a pen at the end of a long series of oscillating circles, shown by the rotating arrows, each subsequent <u>oscillation</u> originating from the tip of the previous circle’s arrow.  The Fourier Transform is the <u>math</u> that can reverse engineer some collection of oscillations, such as music or a painting, and discover the recipe of its various <u>frequencies</u> and their quantities.  By extension, this can apply to 3 (or more) dimensions as well.

## Newton’s 2<sup>nd</sup> Law of Motion

There are a lot of universal <u>patterns</u> and <u>laws</u>, for example, the *Harmonic Series* that describes music, $\frac{1}{1}\frac{1}{2}\frac{1}{3}\frac{1}{4}\frac{1}{5}\frac{1}{6}\frac{1}{7}\frac{1}{8}\cdots$, the *Fibonacci Sequence* of  (0, 1), 1, 2, 3, 5, 8, 12, 21, 34&hellip;, how prime <u>numbers</u> can create &pi;, $\frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\frac{1}{11}+\cdots=\frac{\pi}{4}$, and ratios and constants like *Euler’s <u>number</u>* (*e*, 2.71828), *Phi* (&Phi;, 1.618) and *Pi* (&pi;, 3.14159).  All are at the foundation of many <u>patterns</u> that define reality, and while any two instances of these any one of these <u>laws</u> or ratios may not have a direct connection, they do share the fact that they are a product of that same <u>law</u>.  We don’t say a basketball is the same as a <u>planet</u>, but we do say they are both round, and anything we can say about roundness applies to both.  This may sound childishly simple, but when this is applied to the 2<sup>nd</sup> Law of Motion we see some fascinating <u>patterns</u> and relationships.

Newton’s 2<sup>nd</sup> Law of Motion was the brilliantly simple and profound formula of ***<u>force</u>=<u>mass</u> &times; acceleration***, or ***F=m&times;a***.  This <u>law</u> seems so intuitively obvious that it borders on silly.  We all know that getting hit by a baseball thrown by a little league pitcher might leave a mark, but one thrown by Aroldis Chapman of the Cincinnati Reds might be fatal (Chapman holds the world record of the fastest pitch in history at 105.1 mph in a game against the San Diego Padres on September 24, 2010).  The genius of this formula was not just its simplicity, but that it could be proven that it was as universal as 6=2&times;3 and therefore applied to baseballs as well as the <u>planets</u>, ushering in a revolutionary change in the world view at the time… a world view that others, like <u>Giordano Bruno</u> and <u>Galileo</u>, were burned at the stake or tossed into prison for simply suggesting.

Let’s take a <u>big step back for a moment</u> and look at this as simply as possible. Energy is what causes things to move.  When <u>energy</u> moves, it affects things that it interacts with. We can see this when we drop a rock in the water.  The <u>energy</u> of the falling rock interacts with the water to cause <u>waves</u> to spread out, and we can see how those <u>waves</u> interact with other things, such as the shore.  With each interaction, the <u>energy</u> is dissipated, spread out, shared, until all the <u>energy</u> in the <u>wave</u> has been dissipated, thanks to entropy.

This simple example can be abstracted to three basic <u>concepts</u>: the ***cause*** of <u>movement</u>, the ***medium*** of <u>movement</u> (e.g., <u>mass</u>, <u>electricity</u>, water, etc.) , and the ***effect*** of <u>movement</u> (on that medium).  ***Cause***, ***medium***, ***effect***; these are the properties of the <u>archetypes</u> whose relationships to one another can easily be expressed in the values 6, 2, and 3, respectively.  The 2<sup>nd</sup> Law of Motion tells us that ***<u>mass</u> (2) &times; acceleration (3) =<u>force</u> (6)*** , which implicitly tells us that ***medium (2) &times; effect (3) = cause (6)***, but there’s an obvious yet unspoken property that is fundamental to all 3 properties; ***time***, or more correctly, ***space-time***, as time and space are inextricably bound.

When we diagram this relationship (below) in its simplest form, we can see the natural <u>pattern</u> formed by the relationships between the different instances of <u>energy</u>.  We can also see how this <u>pattern</u> will naturally self-assemble itself into a form.  In this case, the relationships are simple <u>math</u> functions (&times;, &div;).  These functions are themselves <u>patterns</u>, or stable <u>concepts</u>, of the integration of *parts forming a whole* and the disintegration of *a whole forming parts*.  This is why the integration of multiplication (and addition) is bidirectional (i.e., 2&times;3=3&times;2), but the disintegration of division (and subtraction) is one-way (2/3&ne;3/2).  This is 2<sup>nd</sup> grade level <u>math</u>, but it is also the <u>pattern</u> that describes the how reality functions, how parts become wholes and wholes become parts.   The ancient Greeks were extremely well versed in these <u>patterns</u>, but it wasn’t until the16<sup>th</sup> century that Newton applied them to the physical world, bringing us Newton’s Laws, and with them, the modern era of mechanics, physics, relativity, and quantum theory, all of which are based on Newton’s 2<sup>nd</sup> <u>law</u> on Motion, which is based on the <u>pattern</u> of 2&times;3=6.  

<center><img src='../Images/spacetime.png' style='width:100%'/></center>

This is why the same <u>pattern</u> appears in a <u>number</u> of contexts, as shown in the image below, and when seeing these properties in these different contexts, less obvious relationships become clearer.  For example, we see that ***cause*** results from combining ***effect*** and ***medium*** while ***effect*** is the result of separating ***cause*** and ***medium***.  We also see how there is no rational way to explain a ***First Cause*** given that a ***cause*** requires an existing  ***effect*** and ***medium***.  That does not mean there was not a ***First Cause***, just that if there was, it defies <u>reason</u>.  Of course, as mentioned above, the ***First Cause*** could have emerged from an ***effect*** and ***medium*** *outside* our reality, as per Sidis’ idea or the theory that Big Bangs, which create new universes, are actually taking place *inside* a <u>black hole</u>, which, being a maximum state of high-entropy <u>chaos</u>, is *outside* of the Universe that is created by the <u>Big Bang</u>.  Our self-assembled triple-circle <u>pattern</u> agrees, as it quite clearly shows that the ***cause*** in one space-time system was created by the merging of the ***medium*** and ***effect*** of an different space-time system.

<center><img src='../Images/cause-medium-effect-ary.png' style='width:100%'/></center>

We see this same paradox in <u>math</u> in the way that 1 is the effect of 0, because the creation of a <u>concept</u> of nothing demands the creation of a <u>concept</u> of something, or so we are told by philosophers, mathematicians, and physicists[^371].  This makes 0 the ***First Cause***, but it seems contradictory that *nothing* can be the ***First Cause***.  It might make sense if we consider the inside of a <u>black hole</u> as a form of 0, and the <u>singularity</u> that forms within that <u>nothingness</u> as a form of 1, but that might be a bit of a stretch.


[^371]: Carroll, Sean M. "**Why is there something, rather than nothing?.**" *The Routledge Companion to Philosophy of Physics*. Routledge, 2021. 691-706., https://arxiv.org/pdf/1802.02231.pdf

We have seen the 2/3/6 relationships and how they match the ***cause***/***medium***/***effect*** relationships, but there is a 4<sup>th</sup> property of ***space-time***.  How does that fit into the 2/3/6 <u>pattern</u>?  To know this, we have to first determine the value of this new ***space-time*** element.  Fortunately, <u>this is simple to determine as that</u> value has to be the smallest value that all other elements <u>contribute</u> to, and that value is 18, which, being 6&times;3, fits perfectly.  6&times;3  equates to ***<u>force</u> &times; acceleration*** or ***cause &times; effect***.  And what does ***<u>force</u> &times; acceleration*** equal? ***power***.  ***Power*** in this context is defined as “*the rate at which <u>work</u> is done or <u>energy</u> is transferred in a unit of time*".  But wait, isn’t that what ***<u>force</u>*** is, ***<u>energy</u>*** over ***time***?  Yes, but how much <u>energy</u> over how much time?  If it takes me an hour to climb 6 fights of stairs, it requires ***<u>force</u>*** over ***time***, but not much power.  If I ran up 6 flights of stairs in 15 seconds, that would also be a ***<u>force</u>*** over ***time*** but a lot of ***power***.  ***Power*** is a measure of <u>force</u>, and in the world of <u>electricity</u>, this is called ***<u>watts</u>***.  A 100 <u>watt</u> bulb has the same <u>force</u> moving though it as a 1 <u>watt</u> bulb, but 100&times; faster.  Another analogy is how a penny is <u>money</u>.  A wheel barrel of pennies is also the same <u>money</u>, but is has a lot more buying ***power***.  In the physical world, this is called ***pressure***. What do we call it in the archetypal world where ***<u>force</u> &times; acceleration*** is expressed as ***cause &times; effect***?  We could still call it ***power*** or ***pressure*** as the <u>definition</u> remains the same because  ***cause &times; effect*** is still describing the transference of <u>energy</u>, but now with the property of ***space-time***.  In practical terms, ***cause*** and ***effect*** alone, with no regard for the medium or context where things are happening, *is* the ***power*** of this reality, as it is what makes everything happen in this reality.  The ***medium*** will determined the instances of that ***power***, but it is still the same ***power***.

Another context where this <u>law</u> works very well is <u>electricity</u> where the 3 states ***cause***, ***medium***, and ***effect*** are instance of ***<u>volts</u> (V)***, ***<u>resistance</u> (R)***, or ***ohms*** and ***current (I)***, or <u>amperage</u>.  In the electrical world, these relationship are called Ohm’s Law, and just as the 2<sup>nd</sup> <u>law</u> of motion, ***F=m&times;a***, tell us that “*<u>movement</u> is proportional to pressure and inversely proportional to <u>mass</u>*”, Ohm’s <u>law</u>, ***V=R&times;I***, tells us that “*electric current is proportional to <u>voltage</u> and inversely proportional to <u>resistance</u>”. 

The world of matter and <u>electricity</u> <u>share these same qualities</u> in the following ways:

<img src='../Images/matter-elec.png' style='float:right;width:40%'/>**Force** equates to **voltage** (which is technically called *electromotive force*).  Both are of the archetype of **energy**.  **Mass** equates to **resistance** or **ohms**.  Both are of the archetype of **medium** or **resistance**.  **Acceleration** equates to **current** or **amperage** (movement of electrical charge). Both are of the archetype of **effect**. Mechanical **power** equates to electrical **power** as both represent how much energy is transferred in a unit of time and both are of the archetype of **time**.

The commonality of this <u>law</u> is why we can describe the <u>concept</u> of <u>electricity</u> flowing through a wire as water flowing through a pipe.

- **Power (P)** is the rate or measure of water that is being transferred.

- **Current (I)** is the amount of water that is flowing through the pipe.

- **Voltage (V)** is water pressure, which determines how far the water shoots out of the pipe. 

- **Resistance (R)** is represented by the size of the pipe the water is flowing through.

<center><img src='../Images/011-water-example.png' style='width:60%'/></center>

As we move from context to context, such as matter to <u>electricity</u>, these 3 properties are defined, measured, and interact in different ways, but the <u>law</u> does not change, and with a little analysis it can be shown how these different properties relate across many contexts.[^19]

[^19]: Yee, Jeff.  (2019).  **The Relation of Ohm’s Law to Newton’s 2nd Law**.  10.13140/RG.2.2.15576.75523.  https://www.researchgate.net/publication/330639107_The_Relation_of_Ohm%27s_Law_to_Newton%27s_2nd_Law

Here a just some of the common contexts where this <u>law</u> applies:

<center><img src='../Images/012-relatedlaws.png' style='width:100%'/></center>

## The trinity of order

For the record, Ohm’s Law is an instance of what is called the *lumped element model*, which approximates the behavior of a system without having to know the complexity of the underlying systems.  In the case of electricity and Ohm’s Law, we don't need to use the partial differential equations to calculate the Lorenz force to know how many amps the refrigerator needs.  We only need to calculate **I=V/R** (amps = volts/resistance) to know how many amps the fridge uses, rather than<img src='../Images/maxw4.png' style='width:20%'/>.

We saw in the image above titled “*The Pattern*” that <u>energy</u> has 3 states through which it can interact with all other forms of <u>energy</u>, those being **cause**, **effect**, and **medium**. It does not matter if the <u>energy</u> is from a volcano or the <u>flipping of a coin</u>, these 3 states will always be the same.  It’s similar in practice in how the standardized electrical outlets have 3 inputs that <u>work</u> that same no matter where you are or what you plug into them regardless if the <u>energy</u> comes from a coal plant or nuclear power.  Or think of a radio’s power button, volume control, and tuner. No matter what form they take, they <u>work</u> regardless of what’s inside the box, be it tubes, coiled wires, or integrated circuits.  Newton’s <u>laws</u> are the “universal standard” for the mechanical functioning of this part of reality we exist in, but only “this part” of reality.  By “this part” we mean, nothing close to the <u>speed of light</u>, not near a <u>black hole</u> or an enormous <u>mass</u> which distorts spacetime, nothing smaller than an <u>atom</u>, nothing too hot, like the <u>Big Bang</u>, or too cold, like the <u>nothingness</u> of deep space.  Newton’s Laws are valid in middle of the <u>Bell curve</u> where there’s more <u>order</u> and stability&hellip; which is why we exists in that zone as well, and probably why anything we discover as to the true <u>nature</u> and meaning of <u>life</u> is only valid for the zone we live.

## The Patterns

Below are 3 tables that compare Newton’s 2<sup>nd</sup> Law of Motion, the simple <u>math</u> it is based on, and Ohm's Law.

<center><img src='../Images/ohms-fma.png' style='width:100%'/></center>

If we look closely at these formulas, there appears to be at least two that are missing.  We can see that there is a *6&times;3=18* which is ***cause &times; effect = power***, but where is *6&times;2=12,* which is ***cause &times; <u>resistance</u> = ?***.  And where is *3&times;18*, or ***effect &times; power = ?***   What happens if we add them in with the other formulas just to see what all the formulas would look like together?  This is shown below with the new values creatively named *x* and *y*.  

<center><img src='../Images/6laws-3.png' style='width:100%'/></center>

This looks quite different now.  This hexagonal model also shows quite a bit more symmetry and <u>pattern</u> than the classic cube or circle model, and is even self-similar or <u>fractal</u> and has a few other interesting qualities.  For example:

- The bottom values of 12, 18, 36 are the top values of 2, 3, 6 multiplied by 3

- The only value that exists as square of another value is 36 (*<u>force</u><sup>2</sup>*).

- The sum of all the set of top values (2, 3, 6) equals 11 which crosssums to 2 (*<u>mass</u>*), the smallest value of the set and the first of the two prime <u>numbers</u>, and the set of bottom values (12, 18, 36) sum to 66 (*the-crosssum-of-top-numbers&times;6*), which crosssums to 12 (*x*), the smallest of the set, and when crosssummed again gives 3 (*acceleration*), the second of the two prime <u>numbers</u>.  The crosssum of all the <u>numbers</u> gives 77 (*the-crosssum-of-top-numbers&times;7*).

- Multiplying all the <u>numbers</u> together gives 279,936, which is *(2&times;3)<sup>7</sup>*.

- Using the hexagonal model we can arrive at all values using only multiplication and division, while the cube model requires square roots, which hides the greater <u>pattern</u> (described in ‘Family Values’ below)

- The traditional 4-part model has 24 (or *2&times;3&times;4*) possible relationships, but the 6-part model has 720 (2&times;3&times;4&times;5&times;6, the factorial of 6!, or *60&times;12*).  Have you ever wondered why the ancient Sumerians chose 60&times;12 to divide the day, and 60&times;12 to divide the night when they invented the standard of time we still use today?

- Many more interesting <u>patterns</u> that a <u>numberphile</u> could spend days discovering.

The reader may be thinking “Sure, that's what happens naturally with <u>numbers</u>.  There's nothing special about any of this.”, and that would be correct.  Not only that, but the new *x* and *y* values do not tell us anything new from a mathematical perspective.  When you do the <u>math</u>, you discover that *x* is always the same as ***<u>mass</u>*** (i.e., ***medium*** or ***<u>resistance</u>***) , and *y* is always ***<u>force</u><sup>2</sup>*** (i.e., ***cause<sup>2</sup>***)  . OK, so then why are we even bothering with this?  Because we are more interested in the <u>pattern</u> then the practicality of theses extra formulas, and here we show that the *complete* <u>pattern</u> of the model that defines a fundamental <u>law</u> of reality is hexagonal in <u>nature</u>.  This will prove to be an extremely significant detail.  

**Family Values**

Are there only 6 <u>laws</u>? Yes, because 6 is the <u>number</u> of products (results) you get when multiplying any two values (factors) whose products equal a factor *and* where the product is divisible by both factors.  For example, if we start with the factors 2 and 3 we can create the products 4 (*2<sup>2</sup>*,) 6 (*2&times;3*) and 9 (*3<sup>2</sup>*) but 4 and 9 are tossed because only 6 is divisible by both factors of 2 and 3, and are therefore not part of the ‘<u>family</u>’ of 2 and 3  (the bastards). Think of 6 as the child of parents 2 and 3, the 1<sup>st</sup> generation. Now we have a new factor of 6 that can create two more products, 18 (6&times;3) and 12 (*6&times;2*…<u>math</u> is very incestuous) and 36 (*6<sup>2</sup>*). This is the 2<sup>nd</sup> generation.  The total <u>number</u> of values including factors and products is now 6 in <u>number</u>  (2, 3, 6, 12, 18, 36).  In short, the total members of a ‘<u>family</u>’ that descend from both parents will always be the 2 parents plus 1 child for the 1<sup>st</sup> generation plus 3 (inbred) grandchildren for the 2<sup>nd</sup> generation, so that would be *2+1+3=6* (for the curious, there are 19 for the 3<sup>rd</sup> generation).  The reference to children, parents, and <u>family</u> for <u>numbers</u> is not an anthropomorphic metaphor I just came up with, but was adopted from Pythagoras, <u>Plato</u>, and other ancient pioneers of <u>math</u> who took these associations very seriously.  Pythagoras created an entire <u>religion</u> around <u>math</u> on the idea that only <u>math</u> is the one true source of <u>knowledge</u>[^390], and many beliefs and customs were based on <u>math</u>.  This seems odd considering their gods were homicidal inbreeding maniacs, but makes sense if the myths told the same stories of <u>numbers</u> and <u>math</u>, but with a bit more drama.

[^390]: Mankiewicz, Richard. *The Story of Mathematics*. Princeton: University Press, 2004. , pg, 24-26

**A word on <u>electricity</u>…**

We are using Ohm’s <u>law</u>, the <u>laws</u> of <u>electricity</u>, as an example for another <u>reason</u> as well.  We are all familiar with lightning.  Watching the bolts of light shoot instantly across the sky and into (or from) the <u>earth</u> is awe-inspiring and exciting.  Contrary to the statement “Lightning never strikes the same place twice”, lighting often strikes the same place many times, and for good <u>reason</u>.  Ben Franklin suspected why, which gave birth to the lightning rod, saving millions of buildings from burning to the ground.

We know that <u>energy</u> always travels the path of least <u>resistance</u>, so we would say that the path of least <u>resistance</u> for lightning exists between a starting point in the sky and the ending point at a lightning rod (or wherever), and we know that this path was predetermined before the <u>electrons</u> traveled it.  As the opposing charges of the <u>earth</u> and  the sky increases, two fields grow - the positive field of the <u>earth</u> is pulled towards the negative field of the sky, and visa versa.  When they touch, a path is opened and all the excess positive charge rushes towards the sky, and the excess negative charge rushes towards the <u>earth</u> along this path which has already been determined by the fields, similar to how we know where the water form rain will flow, because the paths of least <u>resistance</u> are easily identifiable.  By the time we see the lightning, the fields have already <u>balanced</u> themselves.  The engine of this interaction are the electrical fields.  Lightning is the byproduct of their interaction.  

This <u>pattern</u> that is created by the flow of <u>energy</u> across a field of <u>potential</u> created by the imbalance  between energies can be seen elsewhere, such as in plants, rivers, <u>brain</u> cells, and so many other examples.  Is it reasonable to apply the same logic and suggest that the <u>patterns</u> of roots and rivers are the paths of least <u>resistance</u> that <u>energy</u> travels and are themselves the byproduct, or effect, of interacting fields <u>balancing</u> each other out? If so, what exactly *is* that <u>energy</u> that is <u>balancing</u>?  It seems that the main difference between the <u>pattern</u> of lightning and the <u>pattern</u> of rivers, <u>brain</u> cells, plants, etc., are the energies at play, but energies that take years, centuries or even the lifespan of the universe to <u>balance</u> themselves out.

<center><img src='../Images/lightning-3.png' style='width:100%'/></center>

These bifurcated expanding <u>patterns</u> are ancient, primitive, and foundational in <u>nature</u>.  As <u>nature</u> evolved, so did its <u>patterns</u>, and the <u>patterns</u> created by its own creations.  Perhaps our <u>collective</u> human <u>intelligence</u> is at the *<u>crystallizing</u>* stage of <u>evolution</u>, which for the Universe, was four billion years ago.

<center><img src='../Images/citychip-2.png' style='width:100%'/></center>

**<u>Newtons and Joules</u>**

Before we continue, it would be helpful to clarify how <u>energy</u> is measured, as this will come up a <u>number</u> of times.

The standard measure of <u>energy</u> is a <u>joule</u>.  For example, it takes 1 <u>joule</u> to:

- Run a 1 <u>watt</u> light for 1 second.
- Raise the temperature of 5 drops of water (0.239 grams) from 0&deg; C to 1&deg; C.
- The <u>energy</u> required to accelerate a 1 kg <u>mass</u> at 1 m/s<sup>2</sup> through a distance of 1m.
- The amount of <u>energy</u> your body uses when you are sitting&hellip; 60 times a second, or the amount of <u>energy</u> needed to walk about 8 inches per second.

A <u>joule</u> measures the <u>energy</u> of a <u>force</u>.  And what is a <u>force</u>?  It's something that causes something to change.  if you remember Newton's 1<sup>st</sup> <u>law</u> of motion, the <u>law</u> of inertia, it states that something that is at rest (floating or stationary) will stay that way until some external <u>force</u> is applied to it.

*Newtons* are the units used to describe <u>force</u>, specifically the <u>force</u> required to move 1kg 1 meter per second per second (1m/s<sup>2</sup>).  Imagine A pineapple (which weighs about 1kg)  floating stationary in deep space.  If it were to be pushed with a <u>force</u> of 1 newton it would begin to travel at one meter per second.  If the <u>force</u> was constant, it would accelerate 1m/s<sup>2</sup>, similar to the way things fall at a rate of 9.8m/s<sup>2</sup> here on <u>planet</u> Earth.  This is because gravity exerts a constant <u>force</u> (of 9.8 <u>newtons</u>) on a falling object.  Because a newton is measured in kilograms, and gravity is ~10 <u>newtons</u>, that means that something 100 grams sitting on a table, like a stick of butter, is applying a <u>force</u> of 1 newton to the table.  If you wanted to lift that stick of butter 1m above the table, you would need to apply a <u>force</u> of 1 newton to do so, no matter how long it took.  If it took 1 second, then that is 1 newton/second.  If it took 1 hour, that is 1 newton/hour. 

It takes <u>work</u> to lift that butter, and <u>work</u> takes <u>energy</u>, so how much <u>energy</u> is expended to create that <u>force</u> to lift the butter?  This is where <u>joules</u> come in, as *1 <u>joule</u> = 1 newton&times;1 meter*, or, the amount of <u>energy</u> needed to exert a <u>force</u> of 1 newton on an object to move it 1 meter.  In the butter example, that <u>energy</u> would be 1 <u>joule</u>/sec or 1 <u>joule</u>/hour.  Perhaps the closest commonplace <u>concept</u> to this <u>energy</u>/time/space relationship is the old-fashioned measure of horsepower.  HP was created to compare the <u>work</u> of a steam engine to that of a horse.  If a steam engine could lift a ton of water 1 foot in 1 minute, and it took 4 horses to do the same, then the engine had 4 HP.  This is the same <u>concept</u> behind <u>joules</u>/second, as 1 HP = 745.70 <u>joules</u>/sec.  The Pontiac GTO, a classic muscle car from the 60s, boasted a 300 HP engine, which is equivalent to 223,710 <u>joule</u>/sec.

To sum up:

- Newtons measure the <u>force</u> needed to cause a change.
- Joules measure the <u>energy</u> expended on that change.
- Joules/sec (or HP) measures the <u>energy</u> of change over time.
- Newtons only apply to <u>mass</u>, so this is how we measure things like angular momentum, the <u>force</u> of the <u>planets</u> in orbit, and also how we measure the spin of elementary particles.  
- For things that are not <u>mass</u>, or refer to the <u>energy</u> rather than the <u>force</u> of <u>mass</u>, we would use <u>joules</u>.  

## E=mc<sup>2</sup>

There’s one more <u>pattern</u> comparison to look at.  In the formulas above we can see how the <u>pattern</u> of 18=2&times;3<sup>2</sup> (*P=R&times;I<sup>2</sup>*) looks exactly like another popular formula: *E=m&times;c<sup>2</sup>*.  Using the ‘common sense’ associations, we can equate:

-   ***E*** (<u>energy</u>) to ***P*** (power)
-   ***m*** (<u>mass</u>) to ***R*** (<u>resistance</u>)
-   ***c*** (<u>speed of light</u>, *not* *c<sup>2</sup>*) to ***I*** (current)

We can successfully recreate all 12 formulas from this one equation, making *E=mc<sup>2</sup>* yet another context for this universal <u>pattern</u>. However, a new value that equates to <u>volts</u> or <u>force</u> has appeared.  We’ll call these **zvolts** for now as they equate to electrical <u>volts</u> using the Ohm’s Law formulas.

One interesting observation is how in this context the variable for *speed* or *current* is *c*, the speed of light, and therefore must always remain constant.  It looks like *c,* which is Relativity's version of *current*, or amperage, is the *maximum current supported* rating for this universe, not unlike a 40 Amp fuse we use to ensure we do not melt our wires and burn out our devices.  Does this suggest that if we break the speed of light we would “blow a cosmic fuse” and “melt” our reality? Maybe we'll find out one day. <img src='../Images/014-fuse.png' style='width:60px'/>

But what, if anything, are these **zvolts**? In the world of <u>electricity</u>, <u>voltage</u> is described as *electric pressure* that results from the difference that exists between two states, one being the highest <u>potential</u> <u>energy</u> (like the storm cloud or mountain top), and the other being the lowest <u>potential</u> <u>energy</u> (like the lightning rod or valleys).  In the world of matter, <u>force</u> is the source of pressure that causes change to an object. Can we then say that these missing cosmic <u>volts</u> represent some measure of pressure or <u>force</u> that creates the <u>movement</u> of <u>energy</u> like some sort of relativistic version of <u>volts</u> or forces? 

Let’s calculate the value of a zvolt. If *z=c&times;m*, and *c* is 300,000,000 meters per second, and *m* = 1 gram, then *z=300,000,000 m/s per gram*.  If *m=0*, then *z= 0*. If *m=2* then *z=600,000 m/s per gram*.  So, what is this <u>number</u>?  We know that the <u>energy</u> that is needed to move 1 gram 1 meter is 1 <u>joule</u>, so to move 1 gram 300,000,000 meters you need 300,000,000 <u>joules</u>, and to move 2 grams 300,000,000 meters you need 600,000,000 <u>joules</u> (or 600 megawatts, or 2,682 Pontiac GTOs all traveling at 120 mph).  So, zvolts is <u>joules</u>, which matches perfectly as <u>joules</u> are the <u>energy</u> behind <u>force</u>, and <u>volts</u> are an electrical <u>force</u>.  

## Alchemy

This might seem like an odd place to switch to the subject of <u>alchemy</u>, but it is not, as you will see. 

If we are claiming that these <u>laws</u> and <u>patterns</u> seen throughout our journey of discovery over the past millennia then we should be able to see them in the early forms of reasoning that evolved into such things as modern <u>science</u>.

Alchemy is the birthplace of modern <u>science</u>.  Despite the charlatans of <u>science</u> of olden days, just like today, many alchemists profited by promoting “elixirs of <u>life</u>” and promises of discovering the “philosophers stone”. The true goal of <u>alchemy</u> was to discover the secrets of <u>nature</u>, and to alchemists, this was as much a spiritual journey as it was a technical one.  Modern <u>science</u> has done away with the spiritual or mystical aspects of <u>knowledge</u> and doubled down on the technical aspects (which we will discover is an unsustainable position).

We can see early forms of these modern <u>concepts</u>, specifically in the <u>concepts</u> of the elements of <u>earth</u>, water, air, and fire.  These elements do not refer to the material instances but to their <u>archetypes</u> of which the material forms are limited instances of.  The first form of matter that came into <u>existence</u>, which would be the equivalent of modern <u>science</u>’s soccer-sized ball of everything that exploded to fill the universe, was considered to be formed by these four <u>archetypes</u>.  This was an idea held by the ancient Greeks, the Islamic philosophers and scientists, and learned Asians and Europeans of their day.

As <u>archetypes</u>, they did not only instantiate as matter but also as qualities.  For example, the elements were used to describe <u>health</u> as far back as <u>Hippocrates</u> (400 B.C.), and as recently as Carl <u>Jung</u>’s theory of personality, which drew heavily on <u>Hippocrates</u>.  In <u>Jung</u>’s theory, there were four types of personalities:  feeling (fire, choleric), thinking (water, phlegmatic), intuition (air, sanguine), and sensation (<u>earth</u>, melancholic).  He then added the attributes of introversion/extroversion, to come up with eight basic personality <u>archetypes</u>.

How this is relevant here is that the four alchemical elements are a very early version of the four qualities of matter as expressed by Newton’s 2<sup>nd</sup> Law.  It might seem odd or even ridiculous to compare perhaps the greatest <u>laws</u> of technical thinking to the hocus-pocus of <u>alchemy</u>, but <u>Isaac Newton</u> was himself an alchemist[^329] who not only attempted to turn lead into gold but believed he could discover the Elixir of Life.  In fact, Newton was feared by the English Crown because if he did discover the Philosophers Stone, that magical element that could turn lead into gold, he would ruin the British economy.  Newton also feared the Government as they imposed very severe penalties on anyone trying to turn lead into gold, and for this <u>reason</u> none of his alchemical works were published.  Newton's alchemical <u>work</u> was only discovered in 1936 when his manuscripts were auctioned by Sotheby’s, and it was discovered that one-third of his <u>work</u> was alchemical.  Considering that 20 years of his <u>work</u> was destroyed in a fire started by his dog, one-third might be a very conservative <u>number</u>. These works were labeled "not fit to be printed" by the King after his death for fear someone would pick up where he left off. Today, we have hundreds of years of <u>science</u> to base our thinking on, but before Newton and his <u>laws</u>, there was only <u>alchemy</u>, and this is what Newton studied, along with occultism and hermeticism. Modern <u>science</u> is loath to admit that Newton was as much a ‘magician’ as he was a scientist, and there is no doubt his esoteric studies had an impact on his theory of forces and gravity. 

[^329]: Transcriptions of Newton’s alchemical works are available at https://www.newtonproject.ox.ac.uk/texts/newtons-works/alchemical

What we understand as <u>resistance</u>, current, <u>volts</u>, and power (or <u>mass</u>, velocity, <u>force</u>, and power) today, the alchemists would describe as qualities that have the properties of <u>earth</u>, fire, air, and water <u>archetypes</u>, respectively.  This is not to suggest that just as *F=m&times;a* so too does *air=<u>earth</u> &times;fire*, but we could easily draw parallels of both <u>concepts</u> like <u>movement</u>, <u>energy</u>, <u>force</u>, and <u>resistance</u>.  These different models are an example of how the same <u>patterns</u> and <u>archetypes</u> keep appearing across many contexts and scopes, such as technology, <u>science</u>, mysticism, social <u>order</u>, <u>biology</u>, and many, many more.  In Newton’s case, they formed his <u>understanding</u> of reality that was then applied to his <u>laws</u>.  It seems reasonable to assume that Newton knew of the hexagonal <u>pattern</u> of his <u>laws</u>, but he had no <u>reason</u> to speak of any but the most technically useful. It may also have been the case that he chose to keep certain <u>information</u> and discoveries away from the watchful eyes of both the nervous king and a pope who saw him as a <u>potential</u> heretic.  This, of course, is pure speculation.

<center><img src='../Images/115-elements.png' style='width:80%'/></center>

#### **Key 33:** Instances of <u>laws</u> are limited, defined and understood according to their context.



## Quality of Numbers

Related to <u>concepts</u> of reasoning is how we look at <u>numbers</u>.  Typically, a <u>number</u> is a quantitative value; we have 6 apples, \$1,000 dollars, etc, yet having 6 apples says nothing about the apples themselves.  This lack of qualitative meaning in <u>numbers</u> is at the core of the ongoing debate in the world of statistical analysis.  Imagine the differences in approach and perspective between a quantitative <u>understanding</u> of over population vs. the qualitative <u>understanding</u>.

For clarity, here is how we understand the <u>concepts</u> of *quantitative* and *qualitative*:

- **Quantitative** <u>data</u> can be counted, measured, and expressed using <u>numbers</u>.  Quantitative <u>data</u> has an objective agreed upon value.
- **Qualitative** <u>data</u> that is descriptive and <u>conceptual</u>.  Qualitative <u>data</u> can be categorized based on traits and characteristics.

Lay persons tend not to think of <u>numbers</u> as having qualitative properties.  How would you describe the <u>number</u> 1?  Most people would not say “It’s that <u>number</u> which when multiplied by anything has no effect”, or “It’s its own square root”.  When you see the <u>number</u> 7, how often do you think “I wonder why its inverse is an infinite recurring <u>pattern</u> of all the <u>numbers</u> not divisible by 3, yet add up to 3<sup>3</sup>&times;3?”  (1/7=0.142857 142857 142857 142857 ad *infinitum*).  In ancient times, <u>numbers</u> were far more qualitative; Greeks considered 3 the <u>number</u> of man, 2 of woman, and among the Pythagoreans, 9 was too sacred a <u>number</u> to even be uttered.  Of course, most qualitative values of <u>numbers</u> will be <u>cultural</u>, which doesn’t mean they are invalid, but are only valid within that context, such as how <u>Plato</u> and Socrates, both eugenicists, believed in the qualitative values of <u>numbers</u> when it came to breeding.  <u>Plato</u> even went so far as to suggest that people should only be allowed to listen to certain music based on its harmonics.

However, <u>numbers</u> have a universal, or objective, qualitative value regardless of culture or beliefs.  The Oxford English Dictionary actually defines the word “<u>unity</u>” as “*The abstract quantity representing the <u>singularity</u> of any single entity, regarded as the basis of all whole <u>numbers</u>; **the <u>number</u> one.***”  

Take the <u>number</u> 2, for example.  It is the first pair, and it can be said that “*2 represents the result of two separate things joining to create a new separate thing, the first union*” (we saw an example of this in “Family Values”), or “*It allows us to define an area, which requires 2 dimensions*”.  We can say 3 represents the most stable shape, as the simplest shape that can be created requires 3 sides, or the most stable form is a tripod.  From these qualitative properties we can speculate on the qualitative properties they <u>contribute</u> to the values they can create such as 4, 5, 6, 7, etc.

Further on, when we explore relationships that can be expressed in <u>numbers</u> or geometry, we will often consider their qualitative significance, because to ignore it would be to ignore an entire dimension or perspective of <u>understanding</u>.

## Redundancy

The <u>oscillation</u> constant also gives us a glimpse into another basic, yet profound property of creation and reality, and that is its self-similar <u>redundancy</u>.  *Self-similar <u>redundancy</u>* (a term that is itself redundant) is how one property or <u>law</u> manifests itself across different orders of scale in the most effective manner given the context, state, and <u>scope</u> of that <u>order</u>.

One of the more obvious examples of this type of inter-scope self-similarity might be the commonality between the <u>structure</u> of a solar system and the <u>structure</u> of an <u>atom</u>.

Of course, universal <u>laws</u> such as the <u>laws</u> that describe the conservation of <u>energy</u> and <u>mass</u> apply to all systems, but at the quantum level of an <u>atom</u> we also have local <u>laws</u> such as <u>electron</u> <u>energy</u> levels, the nuclear weak <u>force</u>, etc., and in much larger systems, like a solar system, there are local <u>laws</u> of <u>planetary</u> motion.

Some scientists perceive no relationship between <u>atoms</u> and solar systems, and claim that this analogy depends on an old and outdated <u>concept</u> of the <u>atom</u>, writing it off as humanity’s tendency to oversimplify the complex and over-relate the unrelatable.

No doubt this is true to some degree, but more importantly, there are some similarities worth investigating that would give us an idea of the <u>laws</u> that both systems deploy in the most efficient way they can be expressed, given their <u>scope</u>.  A perfect example of this can be seen in the field of *molecular dynamics* where <u>atoms</u> are treated as a really tiny balls and the bonds between <u>atoms</u> are treated as mechanical springs.  We know <u>atoms</u> and bids are not balls and springs, but applying Newton’s <u>laws</u> to these pretend objects results in surprising accurate predictions.

Naysayers not withstanding, this idea of self-similarity across scopes is legitimate enough to be studied and named.  Quoting from the *International Journal of Theoretical Physics*[^21]:

[^21]: Oldershaw, R.  L.  (1989).  **Self-Similar Cosmological model: Introduction and empirical tests**.  International Journal of Theoretical Physics, 28(6), 669-694.  doi:10.1007/bf00669984 <https://www.academia.edu/26520933/Self-Similar_Cosmological_model_Introduction_and_empirical_tests>

This report concluded:

> The simplicity of [the Self-Similar Cosmological Model (SSCM)] and its ability to quantitatively relate <u>atomic</u>, stellar, and galactic scale phenomena suggest that a new property of <u>nature</u> has been identified: discrete <u>cosmological</u> self-similarity.  Although the SSCM is still in the early heuristic stage of development, it may be the initial step toward a truly remarkable unification of our considerable, but fragmented, physical <u>knowledge</u>.

A more organic example of this inter-scope self-similarity[^22] is to compare the <u>structure</u> of the universe to a <u>brain</u> <u>cell</u>, or the birth of a <u>cell</u> and the death of a star, or the human eye and a nebula, and countless other examples.

[^22]: You can find many examples of this self-similarity <http://www.bordalierinstitute.com>

<center><img src='../Images/brain-eye-nebula.png' style='width:100%'/></center>

There are many matching <u>patterns</u> between cells and the universe, and it is a subject far too broad to get into here.  One recently published paper[^23] shows the similarity in <u>structure</u> of a neutron star and a human <u>cell</u>.  Other comparisons based on scientific and rational observations have also been noted, such as:

[^23]: <https://www.sciencealert.com/scientists-have-found-a-structural-similarity-between-human-cells-and-neutron-stars>

-   Mitochondria vs. stars.
-   Vacuoles vs. galaxies.
-   Nuclear holes vs. <u>asteroids</u>.
-   Vesicles vs. Earth itself.
-   Lysosomes vs. <u>dark-energy</u>.
-   Endoplasmic reticulum vs. wormholes.
-   Cell membrane vs. edge of the universe.
-   Ribosomes vs. molecular clouds.
-   Smooth endoplasmic reticulum vs. the sun.

One could say that if you look long and hard enough you can find relationships and <u>patterns</u> between any two things.  That may be true, but if certain <u>patterns</u> keep popping up then it might be something more than just an overactive imagination.

It might even cause some incurably curious researchers to wonder if there was a bigger picture that they have been ignoring and inspire them to do some investigation that might open new doors of <u>understanding</u>&hellip; someone like the esteemed Stanley N. Salthe, Professor Emeritus, Brooklyn College of the City University of New York, who said:

> It is an interesting possibility that the ‘power <u>laws</u>’ followed by so many different kinds of systems might be the result of downward constraints exerted by encompassing supersystems.  **\~ Stanley N. Salthe, Entropy 2004, 6, 335**

Here is what Hans van Leunen, a physicist from the Eindhoven University of Technology, Dept. of Applied Physics, and founder of *The Hilbert Book Model* project, which applies mathematical test models in <u>order</u> to investigate the foundation of physical reality, has to say about this as well:

> Obviously, physical reality possesses <u>structure</u>, and this <u>structure</u> founds on one or more foundations.  These foundations are rather simple and easily comprehensible.  The major foundation evolves like a <u>seed</u> into more complicated levels of the <u>structure</u>, such that after a series of steps a <u>structure</u> results that appears like the <u>structure</u> of the physical reality that <u>humans</u> can partly observe[^24].  **\~ Hans van Leunen, The Structure of Physical Reality**

[^24]: van Leunen, Hans.  (2018).  **The structure of physical reality**, <https://www.researchgate.net/publication/327273285_The_structure_of_physical_reality>

He then goes on to say:

> The [paper ‘The Structure of Physical Reality’] applies the name *physical reality* to comprise the universe with everything that exists and moves therein.  **It does not matter whether the aspects of this reality are observable.  It is even plausible that a large part of this reality is not in any way perceptible.** The part that is observable shows at the same time an enormous <u>complexity</u>, and yet it demonstrates a peculiarly large <u>coherence</u>.
>
> The conclusion is that physical reality clearly has a <u>structure</u>.  Moreover, this <u>structure</u> has a <u>hierarchy</u>.  Higher layers are becoming more complicated.  That means immediately that a dive into the deeper layers reveals an increasingly simpler <u>structure</u>.  Eventually, we come to the foundation, and that <u>structure</u> must be easily understandable.  The way back to higher <u>structure</u> layers delivers an interesting prospect.  The foundation must <u>force</u> the development of reality in a predetermined direction.  The document postulates that **the <u>evolution</u> of reality resembles the <u>evolution</u> of a <u>seed</u> from which only a specific type of plant can grow.  The growth process provides stringent restrictions so that only this type of plant can develop.  This similarity, therefore, means that the fundamentals of physical reality can only develop the reality that we know**.

In other words, he is saying that there are self-similar and redundant orders in the *<u>hierarchy</u>* and *layers* (in his words) of creation, and these orders abide by specific <u>laws</u> which are limited (*predetermined*) by their component parts (*<u>seeds</u>*).  Likewise, the restrictions of the growth process will be similar at every level, and consequently, the <u>laws</u> at play will be similar.

#### **Key 34:** Reality is a <u>structured</u> <u>hierarchy</u> of dualities which starts out very simple, each specific generation limited by the <u>structures</u> of the duality they emerged from.

You can read his paper[^25], but unless you know your way around multidimensional Hilbert space lattices, it’s going to be a tough read.

[^25]: Leunen, J.  J.  (2018).  **Structure of physical reality**.  <http://vixra.org/pdf/1806.0087v3.pdf>.  Here is the entire report <http://www3.amherst.edu/~rloldershaw/OBS.HTM>

For purposes of this book, I am going to define a  *<u>scope</u>* or *<u>order</u> of creation*, (or *level*, as Hans van Leunen would say), as that creative <u>cycle</u> from which an apparent <u>order</u> emerges out of a state of the apparent disorder defined by the limits of the duality it emerged from.  I say “apparent” because I don’t want to suggest that there is disorder in a <u>seed</u> and <u>order</u> in the resulting flower.  Obviously, there is <u>order</u> in both, but the explicit <u>order</u> of a flower in bloom, at the peak of its expression, when it is ready to drop its own <u>seeds</u>, is far more apparent than the implicit <u>order</u> of a <u>seed</u>.  The flower is *explicit* when it is in bloom, and *implicit* in the <u>seed</u>, while the <u>seed</u> is *implicit* in the flower.  This also suggests that within the <u>scope</u> of the life-cycle of a flower, which begins with a <u>seed</u> and ends with compost, the flowering stage represents the most optimum expression of <u>energy</u>, or the most effective form that instance can realize.

#### **Key 35:** Self-similarity exists due to the redundant <u>nature</u> of the <u>laws</u> which express themselves in accordance with the scale and <u>scope</u> of their context.

