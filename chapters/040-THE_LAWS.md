<div style='page-break-after: always; break-after: always;'></div>

# 4: THE LAWS

###### Energy creates movement, movement follows laws, laws create order, and order directs energy.  Each level of existence expresses the Universal laws according to the context and local laws it exists within.

##### **Synopsis:** If awareness is an expression of energy, then we should expect to see these laws in action, but in a different context. Entropy is as much a philosophical concept as it is a physics concept, as it is the law of balance.  We look at entropy from various perspectives and in relation to chaos, the 2<sup>nd</sup> Law of Thermodynamics, inertia, pattern, and oscillation. These are tied together in Newton’s 2<sup>nd</sup> Law of Motion which is deconstructed to a simple pattern, expanded upon, and re-applied to various contexts such as alchemy, electricity, and relativity.  

##### **Keywords:** energy, entropy, microstates, patterns, systems, Newton, balance

<center><img src='../Images/psolids-3.png' style='width:100%'/></center>

## Entropy

There are many laws that describe how energy works.  From the 3<sup>rd</sup> century B.C. starting with the *Archimedes Principle* to the present laws of quantum mechanics, researchers have been compiling and updating a long list of laws.

One of these universal laws is *energy will always follow the path of least resistance.* This is called *entropy* and explains why water runs downhill to form rivers, why electricity works, why high pressure seeks low pressure, and why stuff breaks.   

In short, entropy measures the amount of energy *not* available for work.  Yes, it sounds (and is) confusing.  It’s like a cashier saying “Here’s your not 70&cent; change” as she hands you 30&cent; from the \$1 you gave her for a 70&cent; chicklet.  There is no such *thing* as entropy, as it measures the lack of something.  The entropy of something is like using someone’s age as a measure of how much life they have left; more equals less.  Why can’t we just measure how much energy something has rather than what it doesn’t have?  For the same reason we can’t start counting down from 82 years to 0 as a measure of how much life someone has left.  We don’t know what the final age of anyone will be, nor do we know what the final energy state of something will be.  So, instead of handing the cashier $1, you just say “charge it”.  She has no idea what you credit balance is, so instead of giving you change, she just adds the 70&cent; to your growing credit card debt. 

**A quick review of entropy&hellip;**

Classical entropy is a thermodynamic concept based on the observations of how heat, or energy, moves.  At its core, it is a measure of how many different ways something can occur, so there is no such *thing* as entropy.  It is a statistical concept only, but it is still real.  You don’t have -\$10, but you do have a debt of \$10.  Entropy is to energy what debt to value.  Using the credit card metaphor, imagine how many ways you can spend money in the Dubai Mall, the largest mall in the world (1,124,000 m<sup>2</sup>, or 157 football fields!).  Now imagine how many ways you can spend money in Mexican Hat, Utah.

<center><img src="../Images/mall.png" style="width:80%"/></center>

Your credit card has extremely low entropy in Mexican Hat, and extremely high entropy in the Dubai mall.  In this metaphor, entropy’s “how many ways something can occur” equates to how many ways you can increase your credit card debt until you are flat broke (high debt, high entropy).  Taking the metaphor further, how much “work” can your &dollar;300 limit credit card do in Dubai?  Not much, as you can maybe get a decent sandwich or a couple pairs of socks.  How much “work” can this same card do at the “last chance” gas station/convenience store for miles and miles in any direction? Quite a bit more, if we define “work” as “obtaining the resources needed to get home”.  On the other hand, let’s say your card had a \&dollar;1,000,000 limit and “work” is defined as “obtaining the resources to live luxuriously”.  Your card can do a lot of “work” in Dubai, but would be useless in Mexican Hat.  OK, we have taken some liberties in this metaphor, but the point is clear.  The ability to do “work” depends on the scope of the “work” and the context of where it is applied. The local entropy of your card is high in Dubai and low in Mexican Hat when “work” means one thing, and opposite when “work” means something else.  However, the *universal* entropy of your card remains the same, very high, as &dollar;300 of potential debt doesn’t do much “work” relative to the 226 trillion in debt floating around in the universe of money.  As black holes account for most of the total entropy in the universe, you can think of the banks and governments that control and own most of this debt as financial black holes.

In a very non-metaphorical example, imagine two simple chemical elements named A and B, each made of 4 particles with 6 bonds that hold them together.  In chemistry, energy is stored in the bonds.  This energy is measured in units called *quanta*.  In A there are 6 quanta, and in B there are 2 quanta, so A is a little hotter than B.  These quanta can move around and be in any arrangement in each object but the energy will stay the same because there will always be 6 quanta in A and 2 quanta in B.  The various configurations that these quanta can have are called *microstates*.

A microstate represents one possible arrangement.  For example, how many microstates exist when tossing 2 pennies?  There are 4:  HH, TH, HT, TT, which means each microstate has a 25% chance.  If the experiment required 10 tosses, then the experiment has 10 *macrostates*, with each macrostate having 4 microstates.  Another example of microstates is a poker hand.   In a deck of 52 cards there are 8.06<sup>67</sup> possible microstates for the deck and  2.6 million possible hands of 5 cards, which is 2.6 million microstates. Of those, only 40 of these microstates are part of the *macrostate* of *straight*-flush while over 1 million are part of the macrostate called *junk* (official poker term for polite society).  Any of the 2.6 million possible microstates is equally likely, but one macrostate has 40 microstates and one has 1,000,000 microstates, so there’s a 25000:1 chance you’ll get dealt a junk hand, thanks to entropy. 

Returning to our example, a new object C is created by putting A and B together.  Now, C has 8 quanta and 12 bonds, so there are many more potential microstates.  But which microstates are the most likely to occur?  The microstates that create the most balanced are the most likely to occur, just like when you untie a balloon, the high-pressure air inside the balloon is more likely to exit the balloon then outside air entering the balloon. 

<center><img src="../Images/microstate_objects.png" style="width:80%"/></center>

Energy will naturally balance itself out simply because there are more microstates available when the quanta is evenly distributed.  This is like saying a head/tail pair is more likely to occur because there are twice as many head/tail microstates because both coins have a head and a tail, or, if you have 10 red buckets and 1 green bucket outside, when it rains, there will be more water in the red buckets than the green.

This example uses the energy in chemical bonds, but energy can be distributed in other ways, such as the radiation emitted by the universe and which we can see in the *cosmic background radiation*.  Where does this energy go?  Nowhere. It just gets recycled because there is nothing to share the energy with outside of this universe.

The formula for entropy looks astonishingly simple, but like that other simple looking formula, *E=mc<sup>2</sup>*, it can get insanely complex.  The change in entropy (*&Delta;S*) is the change in heat (*Q*) divided by temperature (*T*), so *&Delta;S=&Delta;Q/T*.  The *state* of entropy (*S*) is simply the natural log (*ln*) of the number of microstates (*&Omega;*), so *S=ln(&Omega;)*.  In the coin toss example, there are 4 microstates, so *ln(4)=1.386*, or in the case of our elements, we’d use the maximum number of possible microstates per configuration, so *ln(15876)=9.672*.  However, these numbers are useless until we relate them to something real.  In thermodynamics, this number would be multiplied by the *Boltzmann Constant*, *k*, or *k<sub>B</sub>*, which is the ratio of kinetic energy to temperature as measured in joules.  In other areas, like economics, or information theory, that value would be different.  In the conceptual, philosophical, metaphysical, or thought experiment realms, there is no “real” value to apply, but this has more to do with the way we understand what is “real”, because the conceptual, philosophical, metaphysical, and thought experiment realms all use energy in various forms, but not all of those forms are known to us, and measuring them can be difficult, impossible, or meaningless.  

So, what system has the least microstates? A system with the least energy, like something at absolute 0 temperature, which is impossible.  The best we can do is get really, really close (the record so far is 0.0001 Kelvin).  On the other end, what system has the most microstates? The only system that can have all possible microstates in it&hellip; the Universe, which appears to have a finite amount of energy in it. 

To sum up, entropy measures the degree to which the energy of a system is balanced. More entropy means more dispersion of energy, more balancing of differences, low pressure.  Less entropy means less dispersion, more imbalance, more difference, high pressure.

***Continuing&hellip;***

<img src="../Images/grandcanyon.png" style="float:right;width:30%"/>OK, that was the thermodynamic explanation of entropy, and the one most people learn about, but here we will use the word more broadly to reflect a more general concept, which is the ratio *distribution of resources* over *availability of resources*, or $\frac{resource-distribution}{resources-available}$.  So, an abundance of resources with no where to go would be a very **low entropy** of $\frac{1}{1000000}$  vs. few resources that are distributed via many channels, which has a very **high entropy** of $\frac{1000000}{1}$.  In our broader context of the concept of entropy, this is similar to saying $\frac{kinetic-energy}{potential-energy}$ which would be saying that a large river with only 1 possible path, such as the *Grand Canyon*, would be a **low entropy** system, while a trickling stream that empties into a swamp is a **high-entropy** system.

We all know what “work” means when we say “That was a lot of work”, but if someone asked us “How many joules did you expend?”, not knowing the answer doesn’t mean you don’t understand the concept of “work”, and even if you did know the answer, that would have no bearing on the difficulty of that “work”.  Listening to your brother-in-law’s bad poetry reading that you were forced to sit through could be a lot more “work” than building that stone wall around your garden if you have limited “resources” for listening to bad poetry, but many “resources” for building your stone wall even though it requires 87% less joules/hr.

When we speak of *systems*, we are referring to any process that has identifiable boundaries.  In thermodynamics, a system is simply any matter around which a boundary can be drawn, so a rock is a system, and so is a planet or a galaxy.  Poker is a system because it has conceptual boundaries, such as the rules of the game and the cards used to play.  Cars, living organisms, and computers are systems made up of smaller systems that include things like transmissions, organs, and hard-drives. Even a drawing on a piece of paper can be a system as it has boundaries, and inside those boundaries is something that can be described in terms of energy.

The amount of entropy a system has is related to that system.  For example, a lake and a puddle can both have the same entropy because the entropy does not care about size, only relative ratio.  Imagine you are at the top of a mountain with a tank of water.  Let’s say the tank is holding 100 units of water and has 1 faucet for the water to escape. 

- The water in the tank is low entropy because the water just sits there with nowhere to go, i.e., $\frac{nowhere-to-go}{water}=\frac{0}{100}$=0. 

- When you open the faucet, the water has 1 place to go, $\frac{one-place-to-go}{water}=\frac{1}{100}$=0.01. 

- The water that escapes from the tank disperses itself as it follows the path of least resistance down the mountain, or the path of highest probability of microstates which constantly changes as the water rolls over rocks, soaks into the earth, falls over cliffs, etc., where (in this simple example) will eventually arrive to a river, aquifer, lake, or puddle;$\frac{4-places-to-go}{water}=\frac{4}{100}$=0.04.

- Each destination will receive $\frac{1}{4}$ the original amount of water, or 25 units or water per destination.  The destination then becomes another low entropy state of $\frac{nowhere-to-go}{water}=\frac{0}{25}$=0.

Obviously, this is a massively oversimplified and unrealistic example, but it demonstrates the basic concept of how we are applying the idea of  entropy.

It is the movement of energy from low entropy to high entropy that makes “work” possible.  In this example, we could have placed a small water wheel under the faucet which generated electricity to recharge your flashlight, and this water wheel would have added more entropy to the system.

Now, imagine there are 2 identical water tanks on the top of this mountain, one slightly higher than the other. They have the same entropy, yet the higher tank can do more “work” because if we connected the higher tank to the lower tank, we could move water to power a generator, so it must have lower entropy. How can this 2<sup>nd</sup> tank have 2 states of entropy?

Both tanks can actually have many states of entropy, each one based on the scope of the system.  Both tanks have the same *local* entropy, but they both exist inside another larger closed system, the earth, and within that system they have difference states of entropy due to the difference in gravity due to their relative heights.  This difference gives the higher tank lower entropy because it has more pressure do to the constant and accelerating pull of gravity.  If the higher tank was placed in deep space, far from the gravitational field of the earth, yet sill connected to the lower tank on earth, the water would still move between the space-tank and the earth-tank.  Although there is no gravitational pressure, the earth-tank still has a higher entropy because it is in Earth’s gravitational field, which is equivalent to being 4,000 miles from a 1.7 centimeter black hole, and as mention earlier, black holes have a lot of entropy.  In addition, time slows down closer to a black hole, so earth-tank is in a slower, or lower energy zone of space-time than space-tank, and as energy will always travel the path of least resistance, the water will naturally move to a state of lower-energy.

Does this mean that merely by raising the tank higher we can lower its entropy?  Yes, in the *local* sense, but in the *universal* sense, no, because energy is expended in raising the tank higher, pumping the water higher, or, if filled by rain, the effects of gravity (gravity increases entropy, so the lower tank has higher-entropy-rain falling into it) and velocity (the rain entering the lower tank is moving faster, so disperses more energy, raising the entropy), etc. The increase in entropy of the lower tank offsets the decrease in entropy of the higher tank within the scope of the larger system that contains both local tank systems.

This is also why the common argument that evolution breaks the 2<sup>nd</sup> law of thermodynamics is short-sighted.  The argument is that if entropy always leads to disorder, then how can life continually evolve into more ordered forms if entropy is always increasing?  Either the theory of evolution is wrong, or the 2<sup>nd</sup> law  of thermodynamics is wrong.  The confusion comes from the fact that the 2<sup>nd</sup> law only applies to isolated systems, because, obviously, if a system gets energy pumped into it, the entropy will be lowered.  If the poker deck is fattened by 6 additional <img src="../Images/jack-hearts.png" style="height:15px"/>cards at every shuffle, then, yeah, the 2<sup>nd</sup> law no longer applies to the close system of poker. We can easily test this with a simple capacitor that, when external energy is applied to it, collects positive ions on one side and negative ions on the other&hellip; a virtual impossibility in an isolated system where the 2<sup>nd</sup> law applies.  Likewise, the Earth is *not* an isolated system as it receives 5&times;10<sup>23</sup> HP of energy ever second, which is  3.72×10<sup>26</sup> joules (or watts) per second.  That’s a lot of horses galloping to earth from outer space every second bringing 500,000 times more energy than is consumed by humans.  So, while the 2<sup>nd</sup> law applies to the *closed* system of the universe, it doesn’t apply to Earth’s system which is *open* to the solar system. 

An analogy we’ll often use as a common example of entropy is that of a battery.  A charged battery has low entropy (high usable energy), and a dead battery has high entropy (no usable energy).  A dead battery still has all the energy it had before, but it is no longer usable because the energy, in the form of electrons, has moved from the negative side of the battery to the positive side until it achieved balance and there are no more electrons left on the negative side of the battery that were compelled by the laws of balance (entropy) to move&hellip;  but they are still in the battery, and still acting like electrons, but they are “useless” (from a battery’s work perspective).

Let’s return the the poker hand example.  A junk hand, like<img src="../Images/junk.png" style="height:15px"/>, has high entropy because the cards are disordered, or the order is chaotic. On the other hand, <img src="../Images/flush.png" style="height:15px"/> has low entropy as there is order in both suit and value and there is imbalance as all the cards are close in suit and value, which is very rare.  Its not a coincidence that the higher value of a poker hand, the lower the entropy of that hand.

<img src="../Images/zones.png" style="float:right;width:45%"/>The 2<sup>nd</sup> Law of Thermodynamics states “*In an isolated system, entropy never decreases*”.  This is another way to say that all systems tend to move towards their most stable, or low energy states because entropy, or the *inability* to do work, will always increase.  And why won’t it decrease?  Because, left alone, energy, *on average*, will never imbalance itself.  The “on average” part is important, because, if we wanted to get nit-picky about it, there is an extremely small chance, like 1 in 10<sup>24</sup> , that this law can be broken (see generic probability chart on right), but, technically speaking, you could drop an ice cube in hot water and there is an insanely small chance the ice will get colder and the hot water will get hotter, and it may be the first and last time that will ever happen in existence.  The 2<sup>nd</sup> law is not actually a *law*, but a statistical observation, i.e., there is nothing that says that entropy could not be reversed other than the unlikely probability that it can be reversed.  Even so, there can be a spontaneous decrease of entropy[^369], not to mention the hypothetical and highly speculative case put forth that the 2<sup>nd</sup> law exists in reverse somewhere in the Universe[^370], thereby keeping the entropy level of the Universe at 0. It may be easy to simply toss this idea aside until you consider it was written by the man who is considered the smartest human to have ever lived (that we know about.  See *Appendix K, “William James Sedis”*).

[^369]: Xing Xiu-San, “”Spontaneous entropy decrease and its statistical formula”, Department of Physics,Beijing Institute of Technology,Beijing ,China; https://arxiv.org/pdf/0710.4624.pdf
[^370]: Sidis, W.J. (2011). The Animate and the Inanimate.  Originally published in 1920. https://www.sidis.net/animate.pdf

We can apply the concept of entropy to any 2 things that are different and interact with each other.  For example, have you ever wondered why one drop of black paint in a can of white paint makes a big difference, but one drop of white paint in a can of black paint makes little difference?  We can say that white paint has high entropy and black paint has low entropy if we defined the concept of “work” not as kinetic energy but as how much light was reflected or absorbed by a color.  We could measure how much “work” each color performs.  If white paint has an entropy of 1 (absorbs least) and black paint an entropy of 10 (absorbs most), we would discover that if we mix 1 part to 10 of each than the white color changes by 10%, but the black color changes by only 1%.  This application of entropy may work, but it is also confusing because now we are using the same terms and concepts that equates the color black with dispersion, disorder, balanced energy and the color white with pressure, pattern, order, and imbalance.  This is a good example as to the importance of context.  Entropy can be measured as energy radiation in one context, but energy absorption in another.

The universality of the concept of entropy is why it appears in everything from thermodynamics to culture to economics, and anything else that can have microstates, be they chemical, electrical, physical, spiritual, emotional, conceptual, etc..  For example:

-   **In probability theory**, the entropy of a random variable is the measure of uncertainty.
-   **In information theory**, the *compression entropy* of a compressed file, like a zipped file or a JPEG image, measures the amount of information loss.
-   **In sociology**, entropy is the natural decay of a society’s structure (such as law, organization, convention, ethics, etc.).   This also applies to cultures.  Think of the Green Berets of the U.S. Army Special Forces as having low entropy (high ability to work) , and a 1967 free-love hippie commune in Haight-Ashbury as high entropy (low ability to work).  *Note: I have nothing against free-love hippie communes. I have lived on a few and even resided in Haight-Ashbury back in the day.*

We may be taking liberties with the concept of entropy, but even in the science world the definitions of entropy are so diverse and specific that even scientists get confused:

> “As a consequence of this diversity of uses and concepts [of entropy], we may ask whether the use of the term entropy has any meaning.  Is there really something linking this diversity, or is the use of the same term with so many meanings just misleading?”[^13].  **~Annick Lesne, author and researcher at the Institut des Hautes Etudes Scientifiques**

[^13]: Lesne, A.  (2014).  **Shannon entropy: A rigorous notion at the crossroads between probability, information theory, dynamical systems and statistical physics**.  *Mathematical Structures in Computer Science,* *24*(3).  doi:10.1017/s0960129512000783

<center><IMG src="../Images/entropy-trigram.png" style="width:70%"/></center>

For our purposes, we can accept that entropy is not so much a measure of chaos, but a *dimension of chaos*, but where *chaos*, in general, is defined as the *“lack of order in form or movement of energy due to the dispersion and balancing of energy*”.  

For this reason, we defined 2 types of chaos; *high-entropy chaos* and *low-entropy chaos*.  This is the same concept as the *chaos of 0* and the *chaos of &infin;* as mentioned in the first chapter, but here the context is the material reality, not mathematics.

#### **Key 22:** Just as there is the *chaos of 0* and the *chaos of* &infin;, there is *low-entropy chaos* and *high-entropy chaos*.

The image above is a general diagram of this concept.  The 3 patterns in the center show examples of low-entropy (right), high-entropy (left), and a balance of low and high entropy (center).  It’s not a coincident that the center image looks a bit like a plant, as we show later that reality, and living things especially, are a mixture of of order and chaos, high and low entropy combined.  This may be due to the fact that evolution of complex systems actually decreases entropy, while the process of growth itself increases entropy, making life a constant negotiation between these 2 opposite forces.

We can also see that:

- Everything moves from low to high entropy as a result of some sort of dynamic process that dissipates, such as heat in the case of thermodynamics.

- Everything exists on the spectrum of low&rarr;high entropy

- The effect is a result of the cause which becomes a new cause for a new effect.  This is like the cause/effect chain of microstates within microstates; shuffling the deck will result in 1 of 8.06<sup>67</sup>  microstates, and each of those microstates (the hand that was dealt) has 2.6 million microstates. If a hand is part of a favorable macrostate, new microstates will emerge when the holder puts the hand into play.  Another example is how ice is the result of a previous cause/effect chain.  This ice now has it’s own microstates, or *entropic path*, starting out as ice and ending up as vapor, some parts of which ending up in a cloud which will have a different cause/effect path.  Along that path it will be a liquid which will have its own cause/effect path as well, perhaps with its relationship to some salt or sugar, or as hot coffee that gets cold.

<img src='../Images/L-sys-2.png' style='float:right;width:20%'/>Microstates within microstates within microstates… this is equivalent to the previous self-similar fractal pattern of the rainbow bush, but unlike our rainbow bush, the entropic systems branching off one another are chaotic, dynamic systems, so not simply chaos, but an endless chain of chaotic systems within chaotic systems.  But is every system with microstates chaotic?  To some degree, yes, but where the predictability is near 100% it doesn’t *appear* chaotic.  Perhaps we can say it’s only a “little bit” chaotic.  But even the most predictable event only exists at the end of a long chain of chaotic, (previously) unpredictable events.  Consider all the things that had to take place from the first cause to make a predictable coin-toss possible… planets had to be formed, life had to evolve, consciousness and meta-consciousness had to evolve and be “discovered”, etc.  The coin-toss may not be very chaotic, but countless events leading up to it certainly were, which is why everything is part of the chaotic system that is this reality.  

If everything is chaotic, where is the order?  Ironically, order is a subset of chaos, and everything *is* unpredictable to some degree, and that “degree” is measured in *probability*.[^334]  We live our lives as if the sun rising tomorrow is 100% guaranteed, but it isn't.  Sure, there may be only a 1 in 10<sup>-99</sup> chance it won’t rise, but it’s still a probabilistic value.  And the reason this probability is so high is because the solar system has reached a state of equilibrium, a state of high entropy.  Not its highest state of entropy, for when that happens, everything is reduced to cosmic dust and/or black holes, but enough equilibrium to make it sustainable&hellip; for now.  There simply aren’t any more microstates that are more likely than the one we currently live in.  If this equilibrium is disturbed by a huge foreign object passing through our space or a change in temperature of the sun or some other change large enough to create more microstates than currently exist, then the chaos of low-entropy/high-energy will ensue and rearrange everything.  Order emerges when some degree of equilibrium of the parent system is reached.  As the parent system slowly increases its entropy and looses energy, the more energetic, more chaotic, low-entropy child system becomes more stable until it also reaches equilibrium as it loses energy.  Life on earth could not begin until the chaos of Earth’s early days settled down; Earth could not form until the Sun was finished forming in the center of a nebula that was formed by gravity’s effect on dust and gas, etc., etc., all the way back to the Big Bang.  Not one single thing, system, or order exists that is not dependent on the equilibrium of the system it is built upon.  

[^334]: Strevens, Michael. **Bigger than Chaos: Understanding Complexity through Probability**. London: Harvard University Press, 2006. 

Does this mean that one day the probabilities of getting heads in a coin flip will not be 50%?  In one sense yes, because when all matter and energy is equally dispersed, there won’t be any coins, humans, or planets, so while the archetype of that system might exist, the reality of it will not.  It must be stated that this is according to the traditional *Heat Death* theory of how the universe ends.  There are other theories, but in any case, the stable systems will destabilize, and no one will exist to toss coins that don’t exist.  The coin-toss and the coin-tossers both exist because the system of our current reality, the state of reality at this moment in the life-cycle of the Universe, allows for them to exist.  Imagine tossing a coin during the Big Bang, or inside a black hole.  It’s a silly idea because all the laws that allow for tossing and tossers, while they still exist, are uninstantiated and exist only as archetypes.

In our current system, a coin toss is not a random event but one of deterministic chaos.  The reason it is unpredictable is because there are too many micro variables in a toss that effect the outcome; muscle movement, atmosphere, relative starting position, spin axis, initial velocity, imperfections in the coin, whether it’s to be caught or allowed to bounce around when it lands, etc.  A common coin flip, where the coin is not allowed to bounce around when it lands, is a 12 dimensional system, as opposed to the 2-dimensional system of its archetype. This means that while there are only 2 possible outcomes of heads or tails, with 12 dimensions there are 479,001,600 ways to get there.   If we could flip a perfectly vertically balanced coin (because a horizontal coin has a 51% chance of landing in the same orientation as it started because there will always be 1 more even number of flips) in a vacuum with precise pressure on a precise location, and do it identically each time, the result could easily be predicted by Newton’s Law of Motion and angular momentum, at least in theory.  The possibility of creating such a system is (probably) physically impossible, thanks to chaos.  Because the system of a coin toss is a subsystem of countless parent subsystems, not only those of time and space, but those of the tosser, the environment, the coin, etc., all of which are descendants from the initial chaos of creation, any change to any of those parent systems will change the coin toss system.  

Of course, in the quantum world, the coin flip is like a Schrödinger’s Cat scenario, but instead of the cat being in an *eigen state* of dead *and* alive, the unobserved coin is in a eigen state of heads *and* tails, and it is only the observation of the coin that determines which state will collapse into reality.  In this world view, everything that is probabilistic (which is everything that happens), is undetermined until we observe it.  However, as the results of such quantum events results in the same outcomes as the old fashion probabilities and classical physics, we’ll stick to that. [^335]

[^335]: Albrecht, Andreas, and Daniel Phillips. “**Origin of Probabilities and Their Application to the Multiverse.**” *Physical Review D* 90, no. 12 (2014). https://doi.org/10.1103/physrevd.90.123514.  https://arxiv.org/pdf/1212.0953

The laws we live in are, for now, stable enough, but they may be changing as you are reading this.  We know, at least according to our current understanding, that even the most basic laws, like Newton’s Laws of Motion, will completely break down as the universe approaches *statistical equilibrium*, which means there is an equal probability that energy will move in any direction in a reality where all matter, which has decomposed into atomic dust, just wiggles around like 10<sup>86</sup> lost atomic-sized bugs in space as they slowly wait to get sucked into a black hole.  The speed of light, Planck’s Constant, and every other constant, may be changing due to the expansion of the universe, quantum changes, increasing dark-matter, and who knows how many other things we haven’t even discovered yet.  Many scientists have not only theorized this, but there is some evidence to support this idea.  The problem is, on a cosmic time scale (using the new theory that the universe will come to an end in a mere 4 billion years[^333]),  if the universe lasted 100 years, we humans have only been measuring things for the equivalent of 43 millionths of a second (5,000 years), so we can only speculate about the other 99.9999995% of existence.

[^333]: Bousso, Raphael, Ben Freivogel, Stefan Leichenauer, and Vladimir Rosenhaus. “**Eternal Inflation Predicts That Time Will End.**” *Physical Review D* 83, no. 2 (2011). https://doi.org/10.1103/physrevd.83.023525. https://arxiv.org/pdf/1009.4698v1.pdf

#### **Key 23:** Order is high-entropy chaos that exists within high-entropy chaos.

Entropy tells us how close a system is to equilibrium, where perfect equilibrium equals perfect disorder, (because all the parts are spread out equally).  Random dots on a page is perfect equilibrium, and therefore high entropy. If those dots create any sort of pattern, then this is not an equal distribution of dots, and therefore has lower entropy, which means there must be more energy available for “work”.  Exactly what “work” means with regard to information we won’t get into here, but as proof of this, below are the results of the entropy analysis[^332] of 4 images, each with 64,000 black dots on a white background.  You can easily see that more order equals less entropy.

[^332]: Software used: Arch Linux, DiE (detect-it-easy) v3.03, Nov 14, 1021

<center><IMG src="../Images/ent-images.png" style="width:100%"/></center>

So, where does chaos fit into this?

<IMG src="../Images/entropy-hilo.png" style="float:right;width:40%"/>Earlier, we defined 2 kinds of chaos; *high-entropy chaos* and *low-entry chaos*.  We also posited that the most efficient and stable expressions of energy will exist in the middle of these poles.   This naturally divides the spectrum into 2 parts; *ascending order* and *descending order*.  This is shown in the image on the right using our previous model (and flipping the X-axis to match the traditional left &rarr; right x-axis format).  As this is a spectrum of chaos, the *ascending side* that shows a positive growth of order will be called *positive chaos*, and the descending side, which shows negative growth of order is called *negative chaos*.

- **Positive Chaos** (+chaos, low-entropy chaos): The chaos that begins with the single point that represents the totality of somethingness and from there expands.  On a universal scale, this would include the moment of the Big Bang to the moment of maximum order.  On the scale of life, this would represent the time from a germinated seed to a flower.  This describes the *implicate &rarr;explicate* stage of creation. 
- **Negative Chaos** (-chaos, high-entropy chaos): The chaos that begins when the maximum expression of order begins to decline and continues until all order has been disintegrated. Universally, this equates to a dead system, no pattern, no movement of energy, no order of time… just a bunch of dead matter, totally diffused, perfectly balanced, doing nothing.  On a scale of life, it is the journey from flower to compost.  This describes the *explicate &rarr;implicate* stage of creation. 

It would seem more symmetrical and intuitive if the *+chaos* and *-chaos* moved in opposite directions, but that would mean that the *-chaos* would be going backwards in time, which is impossible as that would completely break the 2<sup>nd</sup> Law of Thermodynamics… right?  Not according to the man who has been called “The most intelligent man to ever walk the Earth”.  That might be a bit hyperbolic, and certainly impossible to prove as IQ tests, as we know them today, did exist in his time, but William James Sidis, who spoke 8 languages, wrote 4 academic books by age 8 and was reading the New York Time at 18 months, is alleged  to have had an estimated IQ of 250-300, which is ironic as he considered any sort of intelligence testing as “silly, pedantic, and grossly misleading”.  Still, to put this in perspective, in the entire world today, there is a statistical chance that 1 person on the planet has an IQ of 192.  An IQ of 200 is a 1:76,000,000,000 rarity, so with an IQ of 250+, statistically speaking, he may well have been the smartest human to ever exist (See *Appendix K, “William James Sidis”*, for more)

<IMG src="../Images/entropy-hilo-2.png" style="float:right;width:50%"/>According to Sidis’ highly speculative hypothesis, “*The Animate and the Inanimate*”[^389], published in 1920 when he was 20 years old, entropy could, in theory at least, be reversed, or rather, there exists parts of the Universe that run opposite to ours, like a mirror image of this reality.  These “parts” are interspersed with the “parts” that run “forward”, similar to a checkerboard, but we will never be able to see them or travel to them because they exist is a different space-time.  This sounds like a very early concept of the modern “many worlds” hypothesis of quantum mechanics and is share many of the same concepts of the modern idea that before the Big Bang, the Universe was a reflections of what it is today[^336] .  In this reversed universe, time does not run backwards in the way you might think because its *anti-time* or *negative-time* properties exist in a universe of *anti-matter*, so everything appears the same to those living in *anti-world* as long as all matter, energy, and time were equally inverted.  This is called the *charge, parity, and time (CPT) reversal symmetry*, and it is well understood as *The C.P.T. Theorem*.  If we woke up tomorrow morning in the *anti-world* version of our current reality we live in now, we would not notice anything different, as long as the CPT was inverted.   Applying Sidis’ ideas to our model might look something like the right image, which represents just 2 “squares” on the checkerboard of reversing realities, but this image would also apply to the modern CPT ideas, but where the 0 low-entropy (green) dot would represent the Big Bang.

[^389]:Sidis, W.J. (2011). The Animate and the Inanimate.  Originally published in 1920. https://www.sidis.net/animate.pdf
[^336]: Boyle, Latham, Kieran Finn, and Neil Turok. “The Big Bang, CPT, and Neutrino Dark Matter.” *Annals of Physics* 438 (2022): 168767. https://doi.org/10.1016/j.aop.2022.168767, https://arxiv.org/pdf/1803.08930.pdf

Being of moderate IQ and abilities, I can’t say anything as to the validity or insanity of Sidis’ ideas, but given that Sedis predicted black holes, the expanding universe, and the Big Bang using only the 2<sup>nd</sup> Law of Thermodynamics, and years before the discovery of the expanding universe and the Big Bang, the odds are in his favor.  Regardless, it is not the validity of this idea that is important here, it is the patterns that this idea implies, which is a pattern fundamental to all of existence, from ancient oracles to electricity to DNA, as we’ll see.

As this process of emergence and decay of order applies to any system, then it also applies to not just the life of the Universe, but the life of all the systems created in the Universe, and the systems that those systems create, etc., etc., making reality a dynamic fractal of countless embedded systems of  *low entropy chaos &rarr; order&rarr; high entropy chaos*. 

<center><IMG src="../Images/chaos-chain.png" style="width:70%"/></center>

In this manner, the emergence of an apple follows the same rules as the emergence of the Universe, as does the tree, weather, planet, solar system, etc., that made the existence of that apple possible.  The energy and the archetypes of these rules instantiate within the context and scope of each creation which we recognize as the laws of creation for each context and scope. The example (right image) is a very simple model, but we are following the advice of the award winning quantum physicist Philippe Nozières:

> “Only simple qualitative arguments can reveal the fundamental [laws]”

Because this dispersion of energy is the result of energy always seeking balance, we say that entropy is a measure of the state of balance, as in higher entropy equals more balance, or equilibrium of energy.

#### **Key 24:** Entropy is a measure of balance.

If this is true, then it is also true that:

#### **Key 25:** “Work” is the act of balancing energy and is a result of the imbalance of energy.

“Entropy” is a noun as it only described the state of a system.  What, then, is the verb for the actions that lead to that state?  If entropy is a measure of balance and work is the act of balancing, then “work” is the verb form of entropy.  More work = more balance = more entropy.

Because we will be talking about balance throughout this entire book, let’s make it clear what we mean when we use the word.  “Entropy” and “work” are different words to describe different, yet interdependent concepts, but the single word “balance” is synonymous with both concepts because the word “balance” can be a verb *and* a noun:

- **Balance:** (verb) The application of force required to achieve a stable state of being; Work.
- **Balance:** (noun) The achieved stable state of being; Entropy.

The word itself comes from the Latin *bi+lanx*, meaning “two sauce pans”, as in the classic hanging scales.  The scales show when something is *balanced* (noun) after the two sides were *balanced* (verb).  It will be up to the reader to determine which definition is more contextually appropriate.

Before we leave the subject of entropy, there is another concept to keep in mind that will come up again later:

**Information is entropic.**  This concept was demonstrated in a thought experiment of James Maxwell in 1867 that threatened to break the 2<sup>nd</sup> Law of Thermodynamics.  The gist of this hypothetical was if a magical being of some sort that generated no heat or friction was able to calculate the trajectories of every atom in two chambers, one filled with hot gas and one filled with cold gas, he would be able to selectively allow certain atoms to pass through a door that he could open or close that would make the hot side hotter and the cold side colder.  This was a massive violation of the 2<sup>nd</sup> law, and the inability for anyone to figure out the solution was why Lord Kelvin dubbed this imaginary being *Maxwell’s Demon*.  100 years later, the solution was found: the energy needed for this demon to collect and store all the information on all the atoms would generate enough entropy to more than compensate for the seeming impossible result.  Hence, information itself has entropy, or more correctly, information that is stored.  That storage could be a hard drive, scribbled notes, the brain, or whatever matter the information is stored in/on.  It was also shown that erasing that information raised the entropy even more (in case you were thinking of tricking entropy by only storing a minimum amount of new data and constantly erasing the old data).

#### **Key 26:** Order and pattern requires energy.

The inter-connectivity of things, whether directly or via radiations such as heat and light, allows for movement of energy and this movement creates order.  Therefore, the idea that everything is connected is not simply a philosophical abstract concept, but a necessity for order and therefore life.

#### **Key 27:** Everything is in a state of seeking and/or maintaining balance.

At first, the idea that perfect balance results in perfect chaos seems contradictory, or at least counter-intuitive probably because in our life, and in reality in general, (near) perfect balance only exists in small systems, only temporarily, and under very controlled conditions.  When it does exist, we tend to ignore it because perfect balance doesn't *do* anything, as there is no movement, no energy.  Outside of a laboratory, something that is perfectly balanced is more likely to be tossed in the garbage.  However, if we can extend the idea of balance beyond the classical ideas of movement, we can understand this concept better.  

We know that all of existence is a combination of *systems* within *systems*, from the system of galaxies to the systems of plants and pebbles.  This applies to other contexts as well, such as economics.  When we pay $3 for a pound of carrots we have balanced the difference between a particular instance of supply (of that bag of carrots) and demand (for Bob’s carrot soup).  Once that transaction is complete, there is perfect balance in the very small *system* of that transaction.  That particular instance, being perfectly balanced, no longer has any energy moving through it.  It is a dead systems, and is relegated back to the chaos from which is came.  That instance was but one of many in the larger systems that had to exist for it to even instantiate in the first place; markets, sales, distribution, supply chains, farming, etc., etc., and all the systems they depend on as well to exist. 

We know that, ultimately, these systems exist for a single purpose, the balancing of energy.  We also know that the more microstates there are, the more opportunity there exists for more energy to balance quicker.  This means that the ultimate goal of energy, which is balance, can be enhanced, or optimized, or made more efficient, if there are more microstates, and because it is more efficient, the movement of energy will naturally find a way to create more microstates.  And how will energy create conditions where more microstates will naturally emerge? Through order! Consider the tremendous amount of order on countless levels that must be maintained for you to buy a &dollar;3 bag of carrots from a farm 3000 miles away.

#### **Key 28:** The purpose of order is to facilitate chaos.

Economics, and anything else that we create, conceptually or physically, is as natural a form of evolution as lifeless bio-goo evolving into life, and for the exact same reason.

In the old days, 3.8 billion years ago, when there was plenty of blobs of carbon-based bio-goo, but before life existed, the bio-goo blobs were getting a lot of this free-energy from the sun, the very hot molten core, and the very hot water.  According to the laws of energy, these blobs needed to expend the same amount of energy they took in, but carbon-based blobs have no special energy-dissipation abilities, at least not as blobs.  Energy moving through the blobs, or anything fro that matter, will consequently cause changes that allow for more efficient movement of energy.  Over time, the blobs naturally reorganized to allow better dissipation of excess energy.  Patterns of energy began to form in these blobs, and these patterns became what we have come to know as life.  This is (perhaps) why the earliest forms of life are believed to be 4+ billion year old microorganisms that formed on the hydro-thermal vents on the ocean floor that spew out boiling hot water up to 500&deg;C (930&deg;F).  That heat was their source of energy, and it delivered significantly more energy than the sun (and continues to be a significant contributor to ocean warming as well as methane gas).  The earliest direct evidence of life are the 3.5 billion-year-old microfossils found on the hot water vents in the Pilbara region of Western Australia.

According to Jeremy England, a 31-year-old assistant professor at the Massachusetts Institute of Technology who derived a mathematical formula[^374]that explains how the movement of energy can convert a carbon-based blob into a life form:

>  You start with a random clump of atoms, and if you shine light on it for long enough, it should not be so surprising that you get a plant.

But it’s not just for plants, as we see this also in non-carbon blobs, for example, self-assembling crystals and quasi-crystals.

#### **Key 29:** Life is a natural consequence of the movement of energy for the purpose of moving energy.

[^374]: Jeremy L. England , “”**Statistical physics of self-replication**”, J. Chem. Phys. 139, 121923 (2013) https://doi.org/10.1063/1.4818538

In that first moment the Universe came into being there was the total energy of the Universe in the space of a soccer ball that was released into a void of nothingness.  It was the maximum state of imbalance that began the balancing act of reality. At the end of the Universe, there is no longer any need for energy to balance itself because everything has become balanced. The Universe started with a verb, and ends with a noun.  Wherever there is energy, there is some level of balance, and we say that things are “balanced” when the net energy difference between any two states is 0.  Prior to that difference being 0, there is *balancing*, the movement of energy, and this movement creates order.

#### **Key 30:** Chaos is a state of order, order is a state of chaos.


## Inertia

Another law to look at is Newton’s 1<sup>st</sup> Law of Motion, the Law of Inertia, which states *an object will remain at rest or move at a constant speed in a straight line unless it is acted upon by an unbalanced force*.  This is why things don’t fly around randomly on their own for no reason.  This is mentioned here because it is important to remember that nothing moves, nothing does anything, unless energy is applied as force.  Inertia is to movement what gravity is to things.  Gravity and inertia are two aspects of the same thing as you can calculate the gravitational field of something by how much energy is needed to move it.

These are just two of the many laws that determine how our reality works, at least within the scope of the reality we tend to deal with.  On the quantum and galactic levels, or super high or low energy states, things may operate a bit differently.

Inertia and entropy always ensure that everything will operate at its most efficient level.  What does “operate” mean in this context? It means the optimal movement of energy.  And what does “optimal” mean? It means the most efficient way to balance energy.  As energy only moves when there is a difference between two states, the movement of energy is meant to do only one thing, and that is to minimize that difference by creating a balance between two conditions that are not the same, whether it’s the conditions of *somethingness* and *nothingness* or just a few degrees of temperature.  Once balance is achieved, the movement stops.  A balanced battery is a battery at peace with itself&hellip; and it is also a dead battery.

The optimum condition for the movement of energy between two states is one where both states have maximum expression within the limitations and abilities of both of those states.

<img src='../Images/008-harmonograph.png' style='float:right;width:50%'/>A beautiful example of entropy and inertia that works today as much as it did thousands of years ago can be easily demonstrated with something called the *harmonograph*.  This is a fascinating and entertaining device that creates an oscillation from an initial push (low entropy, high imbalance) and then draws a trace of its path as that initial energy slowly diminishes (inertia, balancing), until it stops (high entropy, balanced).[^16]

[^16]: Here are a couple of sites that show a harmonograph in action.  They are fascinating to watch.  <http://andygiger.com/science/harmonograph/index.html>, <https://www.youtube.com/watch?v=HJYvc-ISrf8>

## Pattern

We recognize that patterns exist in nature, life, physics, math, etc., and it can be useful to think of patterns like rivers that have been etched into the terrain of reality from the first moments of creation and represent a path of least resistance for the movement of energy, be it electrical, physical, conceptual, emotional or otherwise.  Reality, no matter the scope or how one perceives it, is build upon layers and layers of patterns that sit on top of the chaos from which they emerged.  A recurring theme in this book is about recognizing patterns that repeat in various themes, contexts, and scopes.  Some of these meta-patterns are clear, such as the Fibonacci sequence and exponential curves, while others are more hidden and even challenges our ideas of what defines a pattern.  A nice book on this subject is “*Patterns in nature: Why the natural world looks the way it does*” [^17].

[^17]: Ball, P.  (2016).  **Patterns in nature: Why the natural world looks the way it does**.  Chicago: The University of Chicago Press.  ISBN-10: 022633242X ISBN-13: 978-0226332420

The position put forth here is that any two phenomena that share the same pattern are, at least, two instances of one pattern. This differs from the scientific view that the pattern is a result of the cause, and while this makes sense, it also makes sense that it is the archetype of a pattern that is the cause of the instance (form).  It’s really a matter of perspective.

 A Ford Fairlane in Chicago and a Ford Fairlane in Argentina have no direct connection to one another, but some of what is learned about one will apply to the other.  The tricky part is knowing what information is rooted in the patterns versus the instance and its context.  For example, the Ford Fairlane was a “muscle car” and a source of pride among its fans in the U.S.  That same car in Argentina was used by the secret police of the Dictatorship during the “Dirty War” and was a source of fear and intimidation, but an Argentine mechanic could work on that same car in Chicago, even it seeing it filled him with dread.  

While the subjects of this comparison can be defined by patterns of culture, economics, mechanics, and more, the erroneous associations comes from assigning properties of one context’s pattern to another.  This results in superstition and it close cousin, dogma. 

**Superstition**: A belief for which there appears to be no rational substance.

**Dogma**: A belief that people are expected to accept without any doubts.

The sciences are not immune to such confusion.  In fact, modern math was stifled for about a thousand years thanks to the Greek’s superstitious ideas about, and prohibition of, irrational numbers, which were forbidden to be written.  When one of Pythagoras’ students, Hippasus, discovered irrational numbers, he was (depending on your source of history) drowned by the gods for such heresy, or murdered because he revealed how to construct a dodecahedron inside a sphere.  In either case, irrationality and superstition prevailed.  It wasn’t until the early 9<sup>th</sup> century –CE, thanks to Caliph Al-Ma’mun of the Abbasid Caliphate and his *House of Wisdom* in Baghdad (which the Mongols destroyed in the 13<sup>th</sup> century), that mathematics was liberated from its fear of irrational numbers.  We saw earlier that same was true for the number 0, which was banned in Florence in 1299.  Later in this book you will see modern examples of similar thinking in the sciences.

## Oscillation

A law, or pattern, that can be seen in every part of the universe, is that everything oscillates in some manner.  As one of the many definitions of chaos is “*chaos is a kind of order without periodicity*”, there can be chaotic movement of energy, but oscillation allows for a more sustainable movement of energy.  This is why just so many things that exist, from the atoms to the galaxies, oscillate in some fashion and has some kind of frequency.  According to Tesla, this applies to all things that exist: 

> “All things have a frequency and a vibration.”  **\~Nikola Tesla**

#### **Key 31:** Everything exists in a state of duality.

Because everything oscillates 

<img src='../Images/spirals.png' style='float:right;width:50%'/>We typically think of light waves and sound waves as the classical example of oscillations, but the heavenly bodies are also oscillating particles on a cosmic scale.  If we look at the orbits of planets, stars and galaxies they are not simply spinning around in a rather 2D plane of orbits, but that they are spinning around while moving in a direction.

People have been fascinated with this obvious commonality across all of creation for some time.  Kepler himself was quite interested in the relationship between planetary frequencies and musical frequencies, but the study of planetary and musical relationships goes back to at least the 9th century with Eriugena, an Irish monk, theologian, and Neoplatonist philosopher, most famous for his work “The Division of Nature”, which claims that nature’s first primary division was the division between that which **is** (*being* or *somethingness*) and that which is **not** (*nonbeing* or *nothingness*).  His work was condemned as “*swarming with worms of heretical perversity*”.  The 9<sup>th</sup>-century Archdiocese was a tough crowd.

Energy oscillates, and as matter *is* energy, matter also oscillates.  The electrons, protons, and nuclei that constitute all matter are themselves *systems* of oscillating energy fields  When these various *systems* come together to form sustainable patterns of oscillations, they create a new *system*, the atomic structure, which is a high-frequency oscillating energy grid.  For example, a *system* of the nucleus oscillates at ~10<sup>22</sup> Hz.  The *system* of an atom (at 70 &deg;F) oscillates at ~10<sup>15</sup> Hz. An entire molecule, which is a *system* made of *systems* made of *systems*, oscillates at ~10<sup>9</sup> Hz, and so on[^373]. The key ingredient as to what makes a *system* is the sustainability of the oscillations, as that will determine how it integrates into its context and environment.  More significantly, it is the oscillations that are fundamental in the definition of a *system*, as its oscillations define how it will interact with other systems.

[^373]: Source of frequency values: Bentov, Itzhak. “**Stalking the Wild Pendulum: On the Mechanics of Consciousness**”. New York: Bantam Books, 1979. Print.

#### **Key 32:** Oscillation enhances sustainability.

<center><img src='../Images/rotation1.png' style='width:80%'/></center>

*(Image from Grant Sanderson’s video “But what is a Fourier series? From heat flow to circle drawings”*. [^305]

[^305]: at https://www.3blue1brown.com/

On a more abstract level, everything that we can see, hear and touch can be described as a collection of oscillations.  This was proven by the famous French mathematician Joseph Fourier (1768–1830) when he was researching how heat moves.  From his work came the famous and brilliant *Fourier Series* and *Fourier Transformation*.  The Fourier Series describes the hierarchical order of frequencies that would be needed to produce a specific output.  For example, the image above shows a portrait of Joseph Fourier being drawn by a pen at the end of a long series of oscillating circles, shown by the rotating arrows, each subsequent oscillation originating from the tip of the previous circle’s arrow.  The Fourier Transform is the math that can reverse engineer some collection of oscillations, such as music or a painting, and discover the recipe of its various frequencies and their quantities.  By extension, this can apply to 3 (or more) dimensions as well.

## Newton’s 2<sup>nd</sup> Law of Motion

There are a lot of universal patterns and laws, for example, the *Harmonic Series* that describes music, $\frac{1}{1}\frac{1}{2}\frac{1}{3}\frac{1}{4}\frac{1}{5}\frac{1}{6}\frac{1}{7}\frac{1}{8}\cdots$, the *Fibonacci Sequence* of  (0, 1), 1, 2, 3, 5, 8, 12, 21, 34&hellip;, how prime numbers can create &pi;, $\frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\frac{1}{11}+\cdots=\frac{\pi}{4}$, and ratios and constants like *Euler’s number* (*e*, 2.71828), *Phi* (&Phi;, 1.618) and *Pi* (&pi;, 3.14159).  All are at the foundation of many patterns that define reality, and while any two instances of these any one of these laws or ratios may not have a direct connection, they do share the fact that they are a product of that same law.  We don’t say a basketball is the same as a planet, but we do say they are both round, and anything we can say about roundness applies to both.  This may sound childishly simple, but when this is applied to the 2<sup>nd</sup> Law of Motion we see some fascinating patterns and relationships.

Newton’s 2<sup>nd</sup> Law of Motion was the brilliantly simple and profound formula of ***force=mass &times; acceleration***, or ***F=m&times;a***.  This law seems so intuitively obvious that it borders on silly.  We all know that getting hit by a baseball thrown by a little league pitcher might leave a mark, but one thrown by Aroldis Chapman of the Cincinnati Reds might be fatal (Chapman holds the world record of the fastest pitch in history at 105.1 mph in a game against the San Diego Padres on September 24, 2010).  The genius of this formula was not just its simplicity, but that it could be proven that it was as universal as 6=2&times;3 and therefore applied to baseballs as well as the planets, ushering in a revolutionary change in the world view at the time… a world view that others, like Giordano Bruno and Galileo, were burned at the stake or tossed into prison for simply suggesting.

Let’s take a big step back for a moment and look at this as simply as possible. Energy is what causes things to move.  When energy moves, it affects things that it interacts with. We can see this when we drop a rock in the water.  The energy of the falling rock interacts with the water to cause waves to spread out, and we can see how those waves interact with other things, such as the shore.  With each interaction, the energy is dissipated, spread out, shared, until all the energy in the wave has been dissipated, thanks to entropy.

This simple example can be abstracted to three basic concepts: the ***cause*** of movement, the ***medium*** of movement (e.g., mass, electricity, water, etc.) , and the ***effect*** of movement (on that medium).  ***Cause***, ***medium***, ***effect***; these are the properties of the archetypes whose relationships to one another can easily be expressed in the values 6, 2, and 3, respectively.  The 2<sup>nd</sup> Law of Motion tells us that ***mass (2) &times; acceleration (3) =force (6)*** , which implicitly tells us that ***medium (2) &times; effect (3) = cause (6)***, but there’s an obvious yet unspoken property that is fundamental to all 3 properties; ***time***, or more correctly, ***space-time***, as time and space are inextricably bound.

When we diagram this relationship (below) in its simplest form, we can see the natural pattern formed by the relationships between the different instances of energy.  We can also see how this pattern will naturally self-assemble itself into a form.  In this case, the relationships are simple math functions (&times;, &div;).  These functions are themselves patterns, or stable concepts, of the integration of *parts forming a whole* and the disintegration of *a whole forming parts*.  This is why the integration of multiplication (and addition) is bidirectional (i.e., 2&times;3=3&times;2), but the disintegration of division (and subtraction) is one-way (2/3&ne;3/2).  This is 2<sup>nd</sup> grade level math, but it is also the pattern that describes the how reality functions, how parts become wholes and wholes become parts.   The ancient Greeks were extremely well versed in these patterns, but it wasn’t until the16<sup>th</sup> century that Newton applied them to the physical world, bringing us Newton’s Laws, and with them, the modern era of mechanics, physics, relativity, and quantum theory, all of which are based on Newton’s 2<sup>nd</sup> law on Motion, which is based on the pattern of 2&times;3=6.  

<center><img src='../Images/spacetime.png' style='width:100%'/></center>

This is why the same pattern appears in a number of contexts, as shown in the image below, and when seeing these properties in these different contexts, less obvious relationships become clearer.  For example, we see that ***cause*** results from combining ***effect*** and ***medium*** while ***effect*** is the result of separating ***cause*** and ***medium***.  We also see how there is no rational way to explain a ***First Cause*** given that a ***cause*** requires an existing  ***effect*** and ***medium***.  That does not mean there was not a ***First Cause***, just that if there was, it defies reason.  Of course, as mentioned above, the ***First Cause*** could have emerged from an ***effect*** and ***medium*** *outside* our reality, as per Sidis’ idea or the theory that Big Bangs, which create new universes, are actually taking place *inside* a black hole, which, being a maximum state of high-entropy chaos, is *outside* of the Universe that is created by the Big Bang.  Our self-assembled triple-circle pattern agrees, as it quite clearly shows that the ***cause*** in one space-time system was created by the merging of the ***medium*** and ***effect*** of an different space-time system.

<center><img src='../Images/cause-medium-effect-ary.png' style='width:100%'/></center>

We see this same paradox in math in the way that 1 is the effect of 0, because the creation of a concept of nothing demands the creation of a concept of something, or so we are told by philosophers, mathematicians, and physicists[^371].  This makes 0 the ***First Cause***, but it seems contradictory that *nothing* can be the ***First Cause***.  It might make sense if we consider the inside of a black hole as a form of 0, and the singularity that forms within that nothingness as a form of 1, but that might be a bit of a stretch.


[^371]: Carroll, Sean M. "**Why is there something, rather than nothing?.**" *The Routledge Companion to Philosophy of Physics*. Routledge, 2021. 691-706., https://arxiv.org/pdf/1802.02231.pdf

We have seen the 2/3/6 relationships and how they match the ***cause***/***medium***/***effect*** relationships, but there is a 4<sup>th</sup> property of ***space-time***.  How does that fit into the 2/3/6 pattern?  To know this, we have to first determine the value of this new ***space-time*** element.  Fortunately, this is simple to determine as that value has to be the smallest value that all other elements contribute to, and that value is 18, which, being 6&times;3, fits perfectly.  6&times;3  equates to ***force &times; acceleration*** or ***cause &times; effect***.  And what does ***force &times; acceleration*** equal? ***power***.  ***Power*** in this context is defined as “*the rate at which work is done or energy is transferred in a unit of time*".  But wait, isn’t that what ***force*** is, ***energy*** over ***time***?  Yes, but how much energy over how much time?  If it takes me an hour to climb 6 fights of stairs, it requires ***force*** over ***time***, but not much power.  If I ran up 6 flights of stairs in 15 seconds, that would also be a ***force*** over ***time*** but a lot of ***power***.  ***Power*** is a measure of force, and in the world of electricity, this is called ***watts***.  A 100 watt bulb has the same force moving though it as a 1 watt bulb, but 100&times; faster.  Another analogy is how a penny is money.  A wheel barrel of pennies is also the same money, but is has a lot more buying ***power***.  In the physical world, this is called ***pressure***. What do we call it in the archetypal world where ***force &times; acceleration*** is expressed as ***cause &times; effect***?  We could still call it ***power*** or ***pressure*** as the definition remains the same because  ***cause &times; effect*** is still describing the transference of energy, but now with the property of ***space-time***.  In practical terms, ***cause*** and ***effect*** alone, with no regard for the medium or context where things are happening, *is* the ***power*** of this reality, as it is what makes everything happen in this reality.  The ***medium*** will determined the instances of that ***power***, but it is still the same ***power***.

Another context where this law works very well is electricity where the 3 states ***cause***, ***medium***, and ***effect*** are instance of ***volts (V)***, ***resistance (R)***, or ***ohms*** and ***current (I)***, or amperage.  In the electrical world, these relationship are called Ohm’s Law, and just as the 2<sup>nd</sup> law of motion, ***F=m&times;a***, tell us that “*movement is proportional to pressure and inversely proportional to mass*”, Ohm’s law, ***V=R&times;I***, tells us that “*electric current is proportional to voltage and inversely proportional to resistance”. 

The world of matter and electricity share these same qualities in the following ways:

<img src='../Images/matter-elec.png' style='float:right;width:40%'/>**Force** equates to **voltage** (which is technically called *electromotive force*).  Both are of the archetype of **energy**.  **Mass** equates to **resistance** or **ohms**.  Both are of the archetype of **medium** or **resistance**.  **Acceleration** equates to **current** or **amperage** (movement of electrical charge). Both are of the archetype of **effect**. Mechanical **power** equates to electrical **power** as both represent how much energy is transferred in a unit of time and both are of the archetype of **time**.

The commonality of this law is why we can describe the concept of electricity flowing through a wire as water flowing through a pipe.

- **Power (P)** is the rate or measure of water that is being transferred.

- **Current (I)** is the amount of water that is flowing through the pipe.

- **Voltage (V)** is water pressure, which determines how far the water shoots out of the pipe. 

- **Resistance (R)** is represented by the size of the pipe the water is flowing through.

<center><img src='../Images/011-water-example.png' style='width:60%'/></center>

As we move from context to context, such as matter to electricity, these 3 properties are defined, measured, and interact in different ways, but the law does not change, and with a little analysis it can be shown how these different properties relate across many contexts.[^19]

[^19]: Yee, Jeff.  (2019).  **The Relation of Ohm’s Law to Newton’s 2nd Law**.  10.13140/RG.2.2.15576.75523.  https://www.researchgate.net/publication/330639107_The_Relation_of_Ohm%27s_Law_to_Newton%27s_2nd_Law

Here a just some of the common contexts where this law applies:

<center><img src='../Images/012-relatedlaws.png' style='width:100%'/></center>

## The trinity of order

For the record, Ohm’s Law is an instance of what is called the *lumped element model*, which approximates the behavior of a system without having to know the complexity of the underlying systems.  In the case of electricity and Ohm’s Law, we don't need to use the partial differential equations to calculate the Lorenz force to know how many amps the refrigerator needs.  We only need to calculate **I=V/R** (amps = volts/resistance) to know how many amps the fridge uses, rather than<img src='../Images/maxw4.png' style='width:20%'/>.

We saw in the image above titled “*The Pattern*” that energy has 3 states through which it can interact with all other forms of energy, those being **cause**, **effect**, and **medium**. It does not matter if the energy is from a volcano or the flipping of a coin, these 3 states will always be the same.  It’s similar in practice in how the standardized electrical outlets have 3 inputs that work that same no matter where you are or what you plug into them regardless if the energy comes from a coal plant or nuclear power.  Or think of a radio’s power button, volume control, and tuner. No matter what form they take, they work regardless of what’s inside the box, be it tubes, coiled wires, or integrated circuits.  Newton’s laws are the “universal standard” for the mechanical functioning of this part of reality we exist in, but only “this part” of reality.  By “this part” we mean, nothing close to the speed of light, not near a black hole or an enormous mass which distorts spacetime, nothing smaller than an atom, nothing too hot, like the Big Bang, or too cold, like the nothingness of deep space.  Newton’s Laws are valid in middle of the Bell curve where there’s more order and stability&hellip; which is why we exists in that zone as well, and probably why anything we discover as to the true nature and meaning of life is only valid for the zone we live.

## The Patterns

Below are 3 tables that compare Newton’s 2<sup>nd</sup> Law of Motion, the simple math it is based on, and Ohm's Law.

<center><img src='../Images/ohms-fma.png' style='width:100%'/></center>

If we look closely at these formulas, there appears to be at least two that are missing.  We can see that there is a *6&times;3=18* which is ***cause &times; effect = power***, but where is *6&times;2=12,* which is ***cause &times; resistance = ?***.  And where is *3&times;18*, or ***effect &times; power = ?***   What happens if we add them in with the other formulas just to see what all the formulas would look like together?  This is shown below with the new values creatively named *x* and *y*.  

<center><img src='../Images/6laws-3.png' style='width:100%'/></center>

This looks quite different now.  This hexagonal model also shows quite a bit more symmetry and pattern than the classic cube or circle model, and is even self-similar or fractal and has a few other interesting qualities.  For example:

- The bottom values of 12, 18, 36 are the top values of 2, 3, 6 multiplied by 3

- The only value that exists as square of another value is 36 (*force<sup>2</sup>*).

- The sum of all the set of top values (2, 3, 6) equals 11 which crosssums to 2 (*mass*), the smallest value of the set and the first of the two prime numbers, and the set of bottom values (12, 18, 36) sum to 66 (*the-crosssum-of-top-numbers&times;6*), which crosssums to 12 (*x*), the smallest of the set, and when crosssummed again gives 3 (*acceleration*), the second of the two prime numbers.  The crosssum of all the numbers gives 77 (*the-crosssum-of-top-numbers&times;7*).

- Multiplying all the numbers together gives 279,936, which is *(2&times;3)<sup>7</sup>*.

- Using the hexagonal model we can arrive at all values using only multiplication and division, while the cube model requires square roots, which hides the greater pattern (described in ‘Family Values’ below)

- The traditional 4-part model has 24 (or *2&times;3&times;4*) possible relationships, but the 6-part model has 720 (2&times;3&times;4&times;5&times;6, the factorial of 6!, or *60&times;12*).  Have you ever wondered why the ancient Sumerians chose 60&times;12 to divide the day, and 60&times;12 to divide the night when they invented the standard of time we still use today?

- Many more interesting patterns that a numberphile could spend days discovering.

The reader may be thinking “Sure, that's what happens naturally with numbers.  There's nothing special about any of this.”, and that would be correct.  Not only that, but the new *x* and *y* values do not tell us anything new from a mathematical perspective.  When you do the math, you discover that *x* is always the same as ***mass*** (i.e., ***medium*** or ***resistance***) , and *y* is always ***force<sup>2</sup>*** (i.e., ***cause<sup>2</sup>***)  . OK, so then why are we even bothering with this?  Because we are more interested in the pattern then the practicality of theses extra formulas, and here we show that the *complete* pattern of the model that defines a fundamental law of reality is hexagonal in nature.  This will prove to be an extremely significant detail.  

**Family Values**

Are there only 6 laws? Yes, because 6 is the number of products (results) you get when multiplying any two values (factors) whose products equal a factor *and* where the product is divisible by both factors.  For example, if we start with the factors 2 and 3 we can create the products 4 (*2<sup>2</sup>*,) 6 (*2&times;3*) and 9 (*3<sup>2</sup>*) but 4 and 9 are tossed because only 6 is divisible by both factors of 2 and 3, and are therefore not part of the ‘family’ of 2 and 3  (the bastards). Think of 6 as the child of parents 2 and 3, the 1<sup>st</sup> generation. Now we have a new factor of 6 that can create two more products, 18 (6&times;3) and 12 (*6&times;2*…math is very incestuous) and 36 (*6<sup>2</sup>*). This is the 2<sup>nd</sup> generation.  The total number of values including factors and products is now 6 in number  (2, 3, 6, 12, 18, 36).  In short, the total members of a ‘family’ that descend from both parents will always be the 2 parents plus 1 child for the 1<sup>st</sup> generation plus 3 (inbred) grandchildren for the 2<sup>nd</sup> generation, so that would be *2+1+3=6* (for the curious, there are 19 for the 3<sup>rd</sup> generation).  The reference to children, parents, and family for numbers is not an anthropomorphic metaphor I just came up with, but was adopted from Pythagoras, Plato, and other ancient pioneers of math who took these associations very seriously.  Pythagoras created an entire religion around math on the idea that only math is the one true source of knowledge[^390], and many beliefs and customs were based on math.  This seems odd considering their gods were homicidal inbreeding maniacs, but makes sense if the myths told the same stories of numbers and math, but with a bit more drama.

[^390]: Mankiewicz, Richard. *The Story of Mathematics*. Princeton: University Press, 2004. , pg, 24-26

**A word on electricity…**

We are using Ohm’s law, the laws of electricity, as an example for another reason as well.  We are all familiar with lightning.  Watching the bolts of light shoot instantly across the sky and into (or from) the earth is awe-inspiring and exciting.  Contrary to the statement “Lightning never strikes the same place twice”, lighting often strikes the same place many times, and for good reason.  Ben Franklin suspected why, which gave birth to the lightning rod, saving millions of buildings from burning to the ground.

We know that energy always travels the path of least resistance, so we would say that the path of least resistance for lightning exists between a starting point in the sky and the ending point at a lightning rod (or wherever), and we know that this path was predetermined before the electrons traveled it.  As the opposing charges of the earth and  the sky increases, two fields grow - the positive field of the earth is pulled towards the negative field of the sky, and visa versa.  When they touch, a path is opened and all the excess positive charge rushes towards the sky, and the excess negative charge rushes towards the earth along this path which has already been determined by the fields, similar to how we know where the water form rain will flow, because the paths of least resistance are easily identifiable.  By the time we see the lightning, the fields have already balanced themselves.  The engine of this interaction are the electrical fields.  Lightning is the byproduct of their interaction.  

This pattern that is created by the flow of energy across a field of potential created by the imbalance  between energies can be seen elsewhere, such as in plants, rivers, brain cells, and so many other examples.  Is it reasonable to apply the same logic and suggest that the patterns of roots and rivers are the paths of least resistance that energy travels and are themselves the byproduct, or effect, of interacting fields balancing each other out? If so, what exactly *is* that energy that is balancing?  It seems that the main difference between the pattern of lightning and the pattern of rivers, brain cells, plants, etc., are the energies at play, but energies that take years, centuries or even the lifespan of the universe to balance themselves out.

<center><img src='../Images/lightning-3.png' style='width:100%'/></center>

These bifurcated expanding patterns are ancient, primitive, and foundational in nature.  As nature evolved, so did its patterns, and the patterns created by its own creations.  Perhaps our collective human intelligence is at the *crystallizing* stage of evolution, which for the Universe, was four billion years ago.

<center><img src='../Images/citychip-2.png' style='width:100%'/></center>

**Newtons and Joules**

Before we continue, it would be helpful to clarify how energy is measured, as this will come up a number of times.

The standard measure of energy is a joule.  For example, it takes 1 joule to:

- Run a 1 watt light for 1 second.
- Raise the temperature of 5 drops of water (0.239 grams) from 0&deg; C to 1&deg; C.
- The energy required to accelerate a 1 kg mass at 1 m/s<sup>2</sup> through a distance of 1m.
- The amount of energy your body uses when you are sitting&hellip; 60 times a second, or the amount of energy needed to walk about 8 inches per second.

A joule measures the energy of a force.  And what is a force?  It's something that causes something to change.  if you remember Newton's 1<sup>st</sup> law of motion, the law of inertia, it states that something that is at rest (floating or stationary) will stay that way until some external force is applied to it.

*Newtons* are the units used to describe force, specifically the force required to move 1kg 1 meter per second per second (1m/s<sup>2</sup>).  Imagine A pineapple (which weighs about 1kg)  floating stationary in deep space.  If it were to be pushed with a force of 1 newton it would begin to travel at one meter per second.  If the force was constant, it would accelerate 1m/s<sup>2</sup>, similar to the way things fall at a rate of 9.8m/s<sup>2</sup> here on planet Earth.  This is because gravity exerts a constant force (of 9.8 newtons) on a falling object.  Because a newton is measured in kilograms, and gravity is ~10 newtons, that means that something 100 grams sitting on a table, like a stick of butter, is applying a force of 1 newton to the table.  If you wanted to lift that stick of butter 1m above the table, you would need to apply a force of 1 newton to do so, no matter how long it took.  If it took 1 second, then that is 1 newton/second.  If it took 1 hour, that is 1 newton/hour. 

It takes work to lift that butter, and work takes energy, so how much energy is expended to create that force to lift the butter?  This is where joules come in, as *1 joule = 1 newton&times;1 meter*, or, the amount of energy needed to exert a force of 1 newton on an object to move it 1 meter.  In the butter example, that energy would be 1 joule/sec or 1 joule/hour.  Perhaps the closest commonplace concept to this energy/time/space relationship is the old-fashioned measure of horsepower.  HP was created to compare the work of a steam engine to that of a horse.  If a steam engine could lift a ton of water 1 foot in 1 minute, and it took 4 horses to do the same, then the engine had 4 HP.  This is the same concept behind joules/second, as 1 HP = 745.70 joules/sec.  The Pontiac GTO, a classic muscle car from the 60s, boasted a 300 HP engine, which is equivalent to 223,710 joule/sec.

To sum up:

- Newtons measure the force needed to cause a change.
- Joules measure the energy expended on that change.
- Joules/sec (or HP) measures the energy of change over time.
- Newtons only apply to mass, so this is how we measure things like angular momentum, the force of the planets in orbit, and also how we measure the spin of elementary particles.  
- For things that are not mass, or refer to the energy rather than the force of mass, we would use joules.  

## E=mc<sup>2</sup>

There’s one more pattern comparison to look at.  In the formulas above we can see how the pattern of 18=2&times;3<sup>2</sup> (*P=R&times;I<sup>2</sup>*) looks exactly like another popular formula: *E=m&times;c<sup>2</sup>*.  Using the ‘common sense’ associations, we can equate:

-   ***E*** (energy) to ***P*** (power)
-   ***m*** (mass) to ***R*** (resistance)
-   ***c*** (speed of light, *not* *c<sup>2</sup>*) to ***I*** (current)

We can successfully recreate all 12 formulas from this one equation, making *E=mc<sup>2</sup>* yet another context for this universal pattern. However, a new value that equates to volts or force has appeared.  We’ll call these **zvolts** for now as they equate to electrical volts using the Ohm’s Law formulas.

One interesting observation is how in this context the variable for *speed* or *current* is *c*, the speed of light, and therefore must always remain constant.  It looks like *c,* which is Relativity's version of *current*, or amperage, is the *maximum current supported* rating for this universe, not unlike a 40 Amp fuse we use to ensure we do not melt our wires and burn out our devices.  Does this suggest that if we break the speed of light we would “blow a cosmic fuse” and “melt” our reality? Maybe we'll find out one day. <img src='../Images/014-fuse.png' style='width:60px'/>

But what, if anything, are these **zvolts**? In the world of electricity, voltage is described as *electric pressure* that results from the difference that exists between two states, one being the highest potential energy (like the storm cloud or mountain top), and the other being the lowest potential energy (like the lightning rod or valleys).  In the world of matter, force is the source of pressure that causes change to an object. Can we then say that these missing cosmic volts represent some measure of pressure or force that creates the movement of energy like some sort of relativistic version of volts or forces? 

Let’s calculate the value of a zvolt. If *z=c&times;m*, and *c* is 300,000,000 meters per second, and *m* = 1 gram, then *z=300,000,000 m/s per gram*.  If *m=0*, then *z= 0*. If *m=2* then *z=600,000 m/s per gram*.  So, what is this number?  We know that the energy that is needed to move 1 gram 1 meter is 1 joule, so to move 1 gram 300,000,000 meters you need 300,000,000 joules, and to move 2 grams 300,000,000 meters you need 600,000,000 joules (or 600 megawatts, or 2,682 Pontiac GTOs all traveling at 120 mph).  So, zvolts is joules, which matches perfectly as joules are the energy behind force, and volts are an electrical force.  

## Alchemy

This might seem like an odd place to switch to the subject of alchemy, but it is not, as you will see. 

If we are claiming that these laws and patterns seen throughout our journey of discovery over the past millennia then we should be able to see them in the early forms of reasoning that evolved into such things as modern science.

Alchemy is the birthplace of modern science.  Despite the charlatans of science of olden days, just like today, many alchemists profited by promoting “elixirs of life” and promises of discovering the “philosophers stone”. The true goal of alchemy was to discover the secrets of nature, and to alchemists, this was as much a spiritual journey as it was a technical one.  Modern science has done away with the spiritual or mystical aspects of knowledge and doubled down on the technical aspects (which we will discover is an unsustainable position).

We can see early forms of these modern concepts, specifically in the concepts of the elements of earth, water, air, and fire.  These elements do not refer to the material instances but to their archetypes of which the material forms are limited instances of.  The first form of matter that came into existence, which would be the equivalent of modern science’s soccer-sized ball of everything that exploded to fill the universe, was considered to be formed by these four archetypes.  This was an idea held by the ancient Greeks, the Islamic philosophers and scientists, and learned Asians and Europeans of their day.

As archetypes, they did not only instantiate as matter but also as qualities.  For example, the elements were used to describe health as far back as Hippocrates (400 B.C.), and as recently as Carl Jung’s theory of personality, which drew heavily on Hippocrates.  In Jung’s theory, there were four types of personalities:  feeling (fire, choleric), thinking (water, phlegmatic), intuition (air, sanguine), and sensation (earth, melancholic).  He then added the attributes of introversion/extroversion, to come up with eight basic personality archetypes.

How this is relevant here is that the four alchemical elements are a very early version of the four qualities of matter as expressed by Newton’s 2<sup>nd</sup> Law.  It might seem odd or even ridiculous to compare perhaps the greatest laws of technical thinking to the hocus-pocus of alchemy, but Isaac Newton was himself an alchemist[^329] who not only attempted to turn lead into gold but believed he could discover the Elixir of Life.  In fact, Newton was feared by the English Crown because if he did discover the Philosophers Stone, that magical element that could turn lead into gold, he would ruin the British economy.  Newton also feared the Government as they imposed very severe penalties on anyone trying to turn lead into gold, and for this reason none of his alchemical works were published.  Newton's alchemical work was only discovered in 1936 when his manuscripts were auctioned by Sotheby’s, and it was discovered that one-third of his work was alchemical.  Considering that 20 years of his work was destroyed in a fire started by his dog, one-third might be a very conservative number. These works were labeled "not fit to be printed" by the King after his death for fear someone would pick up where he left off. Today, we have hundreds of years of science to base our thinking on, but before Newton and his laws, there was only alchemy, and this is what Newton studied, along with occultism and hermeticism. Modern science is loath to admit that Newton was as much a ‘magician’ as he was a scientist, and there is no doubt his esoteric studies had an impact on his theory of forces and gravity. 

[^329]: Transcriptions of Newton’s alchemical works are available at https://www.newtonproject.ox.ac.uk/texts/newtons-works/alchemical

What we understand as resistance, current, volts, and power (or mass, velocity, force, and power) today, the alchemists would describe as qualities that have the properties of earth, fire, air, and water archetypes, respectively.  This is not to suggest that just as *F=m&times;a* so too does *air=earth &times;fire*, but we could easily draw parallels of both concepts like movement, energy, force, and resistance.  These different models are an example of how the same patterns and archetypes keep appearing across many contexts and scopes, such as technology, science, mysticism, social order, biology, and many, many more.  In Newton’s case, they formed his understanding of reality that was then applied to his laws.  It seems reasonable to assume that Newton knew of the hexagonal pattern of his laws, but he had no reason to speak of any but the most technically useful. It may also have been the case that he chose to keep certain information and discoveries away from the watchful eyes of both the nervous king and a pope who saw him as a potential heretic.  This, of course, is pure speculation.

<center><img src='../Images/115-elements.png' style='width:80%'/></center>

#### **Key 33:** Instances of laws are limited, defined and understood according to their context.



## Quality of Numbers

Related to concepts of reasoning is how we look at numbers.  Typically, a number is a quantitative value; we have 6 apples, \$1,000 dollars, etc, yet having 6 apples says nothing about the apples themselves.  This lack of qualitative meaning in numbers is at the core of the ongoing debate in the world of statistical analysis.  Imagine the differences in approach and perspective between a quantitative understanding of over population vs. the qualitative understanding.

For clarity, here is how we understand the concepts of *quantitative* and *qualitative*:

- **Quantitative** data can be counted, measured, and expressed using numbers.  Quantitative data has an objective agreed upon value.
- **Qualitative** data that is descriptive and conceptual.  Qualitative data can be categorized based on traits and characteristics.

Lay persons tend not to think of numbers as having qualitative properties.  How would you describe the number 1?  Most people would not say “It’s that number which when multiplied by anything has no effect”, or “It’s its own square root”.  When you see the number 7, how often do you think “I wonder why its inverse is an infinite recurring pattern of all the numbers not divisible by 3, yet add up to 3<sup>3</sup>&times;3?”  (1/7=0.142857 142857 142857 142857 ad *infinitum*).  In ancient times, numbers were far more qualitative; Greeks considered 3 the number of man, 2 of woman, and among the Pythagoreans, 9 was too sacred a number to even be uttered.  Of course, most qualitative values of numbers will be cultural, which doesn’t mean they are invalid, but are only valid within that context, such as how Plato and Socrates, both eugenicists, believed in the qualitative values of numbers when it came to breeding.  Plato even went so far as to suggest that people should only be allowed to listen to certain music based on its harmonics.

However, numbers have a universal, or objective, qualitative value regardless of culture or beliefs.  The Oxford English Dictionary actually defines the word “unity” as “*The abstract quantity representing the singularity of any single entity, regarded as the basis of all whole numbers; **the number one.***”  

Take the number 2, for example.  It is the first pair, and it can be said that “*2 represents the result of two separate things joining to create a new separate thing, the first union*” (we saw an example of this in “Family Values”), or “*It allows us to define an area, which requires 2 dimensions*”.  We can say 3 represents the most stable shape, as the simplest shape that can be created requires 3 sides, or the most stable form is a tripod.  From these qualitative properties we can speculate on the qualitative properties they contribute to the values they can create such as 4, 5, 6, 7, etc.

Further on, when we explore relationships that can be expressed in numbers or geometry, we will often consider their qualitative significance, because to ignore it would be to ignore an entire dimension or perspective of understanding.

## Redundancy

The oscillation constant also gives us a glimpse into another basic, yet profound property of creation and reality, and that is its self-similar redundancy.  *Self-similar redundancy* (a term that is itself redundant) is how one property or law manifests itself across different orders of scale in the most effective manner given the context, state, and scope of that order.

One of the more obvious examples of this type of inter-scope self-similarity might be the commonality between the structure of a solar system and the structure of an atom.

Of course, universal laws such as the laws that describe the conservation of energy and mass apply to all systems, but at the quantum level of an atom we also have local laws such as electron energy levels, the nuclear weak force, etc., and in much larger systems, like a solar system, there are local laws of planetary motion.

Some scientists perceive no relationship between atoms and solar systems, and claim that this analogy depends on an old and outdated concept of the atom, writing it off as humanity’s tendency to oversimplify the complex and over-relate the unrelatable.

No doubt this is true to some degree, but more importantly, there are some similarities worth investigating that would give us an idea of the laws that both systems deploy in the most efficient way they can be expressed, given their scope.  A perfect example of this can be seen in the field of *molecular dynamics* where atoms are treated as a really tiny balls and the bonds between atoms are treated as mechanical springs.  We know atoms and bids are not balls and springs, but applying Newton’s laws to these pretend objects results in surprising accurate predictions.

Naysayers not withstanding, this idea of self-similarity across scopes is legitimate enough to be studied and named.  Quoting from the *International Journal of Theoretical Physics*[^21]:

[^21]: Oldershaw, R.  L.  (1989).  **Self-Similar Cosmological model: Introduction and empirical tests**.  International Journal of Theoretical Physics, 28(6), 669-694.  doi:10.1007/bf00669984 <https://www.academia.edu/26520933/Self-Similar_Cosmological_model_Introduction_and_empirical_tests>

This report concluded:

> The simplicity of [the Self-Similar Cosmological Model (SSCM)] and its ability to quantitatively relate atomic, stellar, and galactic scale phenomena suggest that a new property of nature has been identified: discrete cosmological self-similarity.  Although the SSCM is still in the early heuristic stage of development, it may be the initial step toward a truly remarkable unification of our considerable, but fragmented, physical knowledge.

A more organic example of this inter-scope self-similarity[^22] is to compare the structure of the universe to a brain cell, or the birth of a cell and the death of a star, or the human eye and a nebula, and countless other examples.

[^22]: You can find many examples of this self-similarity <http://www.bordalierinstitute.com>

<center><img src='../Images/brain-eye-nebula.png' style='width:100%'/></center>

There are many matching patterns between cells and the universe, and it is a subject far too broad to get into here.  One recently published paper[^23] shows the similarity in structure of a neutron star and a human cell.  Other comparisons based on scientific and rational observations have also been noted, such as:

[^23]: <https://www.sciencealert.com/scientists-have-found-a-structural-similarity-between-human-cells-and-neutron-stars>

-   Mitochondria vs. stars.
-   Vacuoles vs. galaxies.
-   Nuclear holes vs. asteroids.
-   Vesicles vs. Earth itself.
-   Lysosomes vs. dark-energy.
-   Endoplasmic reticulum vs. wormholes.
-   Cell membrane vs. edge of the universe.
-   Ribosomes vs. molecular clouds.
-   Smooth endoplasmic reticulum vs. the sun.

One could say that if you look long and hard enough you can find relationships and patterns between any two things.  That may be true, but if certain patterns keep popping up then it might be something more than just an overactive imagination.

It might even cause some incurably curious researchers to wonder if there was a bigger picture that they have been ignoring and inspire them to do some investigation that might open new doors of understanding&hellip; someone like the esteemed Stanley N. Salthe, Professor Emeritus, Brooklyn College of the City University of New York, who said:

> It is an interesting possibility that the ‘power laws’ followed by so many different kinds of systems might be the result of downward constraints exerted by encompassing supersystems.  **\~ Stanley N. Salthe, Entropy 2004, 6, 335**

Here is what Hans van Leunen, a physicist from the Eindhoven University of Technology, Dept. of Applied Physics, and founder of *The Hilbert Book Model* project, which applies mathematical test models in order to investigate the foundation of physical reality, has to say about this as well:

> Obviously, physical reality possesses structure, and this structure founds on one or more foundations.  These foundations are rather simple and easily comprehensible.  The major foundation evolves like a seed into more complicated levels of the structure, such that after a series of steps a structure results that appears like the structure of the physical reality that humans can partly observe[^24].  **\~ Hans van Leunen, The Structure of Physical Reality**

[^24]: van Leunen, Hans.  (2018).  **The structure of physical reality**, <https://www.researchgate.net/publication/327273285_The_structure_of_physical_reality>

He then goes on to say:

> The [paper ‘The Structure of Physical Reality’] applies the name *physical reality* to comprise the universe with everything that exists and moves therein.  **It does not matter whether the aspects of this reality are observable.  It is even plausible that a large part of this reality is not in any way perceptible.** The part that is observable shows at the same time an enormous complexity, and yet it demonstrates a peculiarly large coherence.
>
> The conclusion is that physical reality clearly has a structure.  Moreover, this structure has a hierarchy.  Higher layers are becoming more complicated.  That means immediately that a dive into the deeper layers reveals an increasingly simpler structure.  Eventually, we come to the foundation, and that structure must be easily understandable.  The way back to higher structure layers delivers an interesting prospect.  The foundation must force the development of reality in a predetermined direction.  The document postulates that **the evolution of reality resembles the evolution of a seed from which only a specific type of plant can grow.  The growth process provides stringent restrictions so that only this type of plant can develop.  This similarity, therefore, means that the fundamentals of physical reality can only develop the reality that we know**.

In other words, he is saying that there are self-similar and redundant orders in the *hierarchy* and *layers* (in his words) of creation, and these orders abide by specific laws which are limited (*predetermined*) by their component parts (*seeds*).  Likewise, the restrictions of the growth process will be similar at every level, and consequently, the laws at play will be similar.

#### **Key 34:** Reality is a structured hierarchy of dualities which starts out very simple, each specific generation limited by the structures of the duality they emerged from.

You can read his paper[^25], but unless you know your way around multidimensional Hilbert space lattices, it’s going to be a tough read.

[^25]: Leunen, J.  J.  (2018).  **Structure of physical reality**.  <http://vixra.org/pdf/1806.0087v3.pdf>.  Here is the entire report <http://www3.amherst.edu/~rloldershaw/OBS.HTM>

For purposes of this book, I am going to define a  *scope* or *order of creation*, (or *level*, as Hans van Leunen would say), as that creative cycle from which an apparent order emerges out of a state of the apparent disorder defined by the limits of the duality it emerged from.  I say “apparent” because I don’t want to suggest that there is disorder in a seed and order in the resulting flower.  Obviously, there is order in both, but the explicit order of a flower in bloom, at the peak of its expression, when it is ready to drop its own seeds, is far more apparent than the implicit order of a seed.  The flower is *explicit* when it is in bloom, and *implicit* in the seed, while the seed is *implicit* in the flower.  This also suggests that within the scope of the life-cycle of a flower, which begins with a seed and ends with compost, the flowering stage represents the most optimum expression of energy, or the most effective form that instance can realize.

#### **Key 35:** Self-similarity exists due to the redundant nature of the laws which express themselves in accordance with the scale and scope of their context.

